{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9327b72d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:16:12.592147Z",
     "start_time": "2024-10-30T18:16:12.588652Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import uuid\n",
    "import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f98dc722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:16:15.013821Z",
     "start_time": "2024-10-30T18:16:15.009440Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "source_dir = \"source_files/\"\n",
    "source_metadata_file = source_dir + \"papers.csv\"\n",
    "processed_metadata_file = source_dir + \"papers_processed.csv\"\n",
    "\n",
    "table_output_dir = \"extracted_tables/\"\n",
    "table_code_dir = table_output_dir + \"table_code/\"\n",
    "table_result_file = table_output_dir + \"tables.csv\"\n",
    "\n",
    "figure_output_dir = \"extracted_figures/\"\n",
    "figure_result_file = figure_output_dir + \"figures.csv\"\n",
    "\n",
    "possible_extensions = [\".pdf\", \".png\", \".jpg\", \".jpeg\", \".eps\"]\n",
    "\n",
    "os.makedirs(table_output_dir, exist_ok=True)\n",
    "os.makedirs(table_code_dir, exist_ok=True)\n",
    "os.makedirs(figure_output_dir, exist_ok=True)\n",
    "\n",
    "if os.path.isfile(table_result_file) == False:\n",
    "    open(table_result_file, \"w\").close()\n",
    "    \n",
    "if os.path.isfile(figure_result_file) == False:\n",
    "    open(figure_result_file, \"w\").close()\n",
    "    \n",
    "if os.path.isfile(processed_metadata_file) == False:\n",
    "    open(processed_metadata_file, \"w\").close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf3937e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:16:17.037975Z",
     "start_time": "2024-10-30T18:16:16.989352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset directories and csv files\n",
    "# WARNING: This deletes all collected tables and figures\n",
    "files = glob.glob(table_code_dir + \"*\")\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "    \n",
    "files = glob.glob(figure_output_dir + \"*\")\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "open(figure_result_file, \"w\").close()\n",
    "\n",
    "if os.path.isfile(table_result_file):\n",
    "    os.remove(table_result_file)\n",
    "    open(table_result_file, \"w\").close()\n",
    "    \n",
    "if os.path.isfile(processed_metadata_file):\n",
    "    os.remove(processed_metadata_file)\n",
    "    open(processed_metadata_file, \"w\").close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4632c02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Returns a list of papers that were downloaded but not yet processed.\n",
    "\"\"\"\n",
    "def get_unprocessed_papers():\n",
    "    csvfile_source = open(source_metadata_file, \"r\")\n",
    "    csvfile_processed = open(processed_metadata_file, \"r\")\n",
    "    source_spamreader = csv.reader(csvfile_source, delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "    processed_spamreader = csv.reader(csvfile_processed, delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "    \n",
    "    processed_papers = [row[0] for row in processed_spamreader]\n",
    "    unprocessed_papers = []\n",
    "    \n",
    "    for row in source_spamreader:\n",
    "        if row[0] not in processed_papers:\n",
    "            unprocessed_papers.append(row)\n",
    "            \n",
    "    csvfile_source.close()\n",
    "    csvfile_processed.close()\n",
    "    \n",
    "    return unprocessed_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e88978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:23:10.605702500Z",
     "start_time": "2024-10-30T18:16:19.958294Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Extracting tables and figures from the downloaded papers in the source directory\n",
    "\"\"\"\n",
    "def extract_data_from_papers(unprocessed_papers, amount):\n",
    "    for i in range(amount):\n",
    "        row = unprocessed_papers[i]\n",
    "        \n",
    "        paper_id = row[0]\n",
    "        paper_path = source_dir + paper_id\n",
    "        if os.path.isdir(paper_path):\n",
    "            print(\"\\n\\n\" + paper_id)\n",
    "            tex_files = [os.path.join(root, file)\n",
    "                 for root, dirs, files in os.walk(paper_path)\n",
    "                 for file in files if file.endswith(\".tex\") and file.startswith(\"FR_\") == False]\n",
    "            print(tex_files)\n",
    "\n",
    "            complete_tex = \"\"\n",
    "            for file in tex_files:\n",
    "                try:\n",
    "                    f = open(file, \"r\", encoding=\"utf8\")\n",
    "                    complete_tex += f.read()\n",
    "                    f.close()\n",
    "                except UnicodeDecodeError as e:\n",
    "                    print(\"UnicodeDecodeError occurred. File could not be loaded.\")\n",
    "                    continue\n",
    "\n",
    "            found_tables = re.findall(r\"\\\\begin\\{table\\*?\\}.*?\\\\end\\{table\\*?\\}\", complete_tex, re.DOTALL)\n",
    "            found_figures = re.findall(r\"\\\\begin\\{figure\\*?\\}.*?\\\\end\\{figure\\*?\\}\", complete_tex, re.DOTALL)\n",
    "            counter_table = 0\n",
    "            counter_figure = 0\n",
    "            \n",
    "            # Capture document preamble. If it can't be captured, skip table capturing.\n",
    "            found_document_header = re.search(r\"\\\\documentclass\\[.*?\\\\begin\\{document\\}\", complete_tex, re.DOTALL)\n",
    "            if found_document_header is None:\n",
    "                print(\"Document header could not be identified.\")\n",
    "                found_tables = []\n",
    "\n",
    "            # Table capturing\n",
    "            for table in found_tables:\n",
    "                #r\"\\\\caption\\{(.*?)}\"\n",
    "                #caption_match = re.search(r\"\\\\caption\\{(([^{}]*(\\{[^{}]*\\})?[^{}]*)+)\\}\", table)\n",
    "\n",
    "                label_match = re.search(r\"\\\\label\\{(.*?)\\}\", table)\n",
    "                caption_match = re.search(r\"\\\\caption\\{(.*?)}\", table)\n",
    "                \n",
    "                if label_match and caption_match:\n",
    "                    \n",
    "                    # Skipping if no paragraph mention was found\n",
    "                    found_label = label_match.group(1)\n",
    "                    found_paragraphs = re.findall(fr\".*\\\\ref\\{{{re.escape(found_label)}}}.*\", complete_tex)\n",
    "                    if len(found_paragraphs) == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    paragraph_mentions = \"\"\n",
    "                    for paragraph in found_paragraphs:\n",
    "                        paragraph_mentions += paragraph\n",
    "      \n",
    "                    found_caption = caption_match.group(1)\n",
    "                    counter_table += 1\n",
    "                    table_id = paper_id + \"_TAB_\" + str(counter_table)\n",
    "\n",
    "                    # Store meta information in csv file\n",
    "                    with open(table_result_file, 'a', newline='', encoding=\"utf-8\") as csvfile_table:\n",
    "                        csvfile_table.write(table_id + \";\" + paper_id + \";https://arxiv.org/abs/\" + row[0] + \";\" + found_caption + \";\" + paragraph_mentions + \"\\n\")\n",
    "\n",
    "                    # Store as txt file in own data collection\n",
    "                    table_file_path = table_code_dir + table_id + \".txt\"\n",
    "                    f = open(table_file_path, \"w\", encoding=\"utf-8\")\n",
    "                    f.write(table)\n",
    "                    f.close()\n",
    "\n",
    "                    # Store as tex file in folder of paper\n",
    "                    table_file_path = paper_path + \"/FR_TAB_\" + table_id + \".tex\"\n",
    "                    table_tex_code = found_document_header.group(0) + \"\\n\\pagenumbering{gobble}\\n\" + table + \"\\n\\end{document}\"\n",
    "                    f = open(table_file_path, \"w\", encoding=\"utf-8\")\n",
    "                    f.write(table_tex_code)\n",
    "                    f.close()\n",
    "\n",
    "            # Figure capturing\n",
    "            for figure in found_figures:\n",
    "                found_graphics = re.findall(r\"\\\\includegraphics(\\[.*?\\])*\\{(.*?)\\}\", figure)\n",
    "                #caption_match = re.search(r\"\\\\caption\\{(([^{}]*(\\{[^{}]*\\})?[^{}]*)+)\\}\", figure)\n",
    "                caption_match = re.search(r\"\\\\caption\\{(.*?)}\", figure)\n",
    "                label_match = re.search(r\"\\\\label\\{(.*?)\\}\", figure)\n",
    "                \n",
    "                # Skipping multi figures\n",
    "                if len(found_graphics) != 1:\n",
    "                    continue\n",
    "                \n",
    "                # Caption and label need to be found\n",
    "                if caption_match and label_match:                  \n",
    "                    \n",
    "                    # Skipping if no paragraph mention was found\n",
    "                    found_label = label_match.group(1)\n",
    "                    found_paragraphs = re.findall(fr\".*\\\\ref\\{{{re.escape(found_label)}}}.*\", complete_tex)\n",
    "                    if len(found_paragraphs) == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    paragraph_mentions = \"\"\n",
    "                    for paragraph in found_paragraphs:\n",
    "                        paragraph_mentions += paragraph\n",
    "                    paragraph_mentions.replace(\";\", \",\")\n",
    "                    \n",
    "                    found_caption = caption_match.group(1)\n",
    "                    graphic = found_graphics[0]\n",
    "\n",
    "                    graphic_path = graphic[1]\n",
    "                    file_type = os.path.splitext(graphic_path)[-1]\n",
    "                    if os.path.isfile(paper_path+\"/\"+graphic_path):\n",
    "                        graphic_path = paper_path+\"/\"+graphic_path\n",
    "                    else:\n",
    "                        unknown_extension = True\n",
    "                        for ext in possible_extensions:\n",
    "                            possible_path = paper_path+\"/\"+graphic_path + ext\n",
    "                            if os.path.isfile(possible_path):\n",
    "                                graphic_path = possible_path\n",
    "                                file_type = ext\n",
    "                                unknown_extension = False\n",
    "                                break\n",
    "                        if unknown_extension:\n",
    "                            continue\n",
    "\n",
    "                    try:\n",
    "                        counter_figure += 1\n",
    "                        figure_id = paper_id + \"_FIG_\" + str(counter_figure)\n",
    "                        shutil.copy(graphic_path, figure_output_dir+figure_id+file_type)\n",
    "                        with open(figure_result_file, 'a', newline='', encoding=\"utf-8\") as csvfile_figure:\n",
    "                            csvfile_figure.write(figure_id + \";\" + row[0] + \";https://arxiv.org/abs/\" + row[0] + \";\" + found_caption + \";\" + paragraph_mentions + \"\\n\")\n",
    "                    except FileNotFoundError as e:\n",
    "                        print(f\"File not found: {graphic_path} - {e}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred: {e}\")\n",
    "\n",
    "            number_tables = len(found_tables)\n",
    "            number_figures = len(found_figures)\n",
    "            print(f\"{counter_table}/{number_tables} tables collected.\")\n",
    "            print(f\"{counter_figure}/{number_figures} figures collected.\")\n",
    "            \n",
    "            finish_process(row, counter_table, counter_figure)\n",
    "        else:\n",
    "            print(f\"Paper {paper_id} could not be found.\")\n",
    "            finish_process(row, -1, -1)\n",
    "                \n",
    "# Store meta information in processed_papers csv file and deletes paper from disk if it is not longer needed\n",
    "def finish_process(row, number_tables, number_figures):\n",
    "    \n",
    "    # Check if there are papers that still need to be processed (as image file)\n",
    "    tables_to_be_process = True\n",
    "    if number_tables <= 0:\n",
    "        tables_to_be_process = False\n",
    "    \n",
    "    # Write into processed_papers csv file\n",
    "    with open(processed_metadata_file, \"a\", newline='', encoding=\"utf-8\") as csvfile_processed:\n",
    "        spamwriter = csv.writer(csvfile_processed, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        row.extend([str(number_tables), str(number_figures), str(tables_to_be_process)])\n",
    "        spamwriter.writerow(row)\n",
    "\n",
    "    # Delete paper if no tables were found (because no table image extraction is needed)\n",
    "    if tables_to_be_process == False:\n",
    "        try:\n",
    "            shutil.rmtree(source_dir + row[0])\n",
    "        except Exception as e:\n",
    "            print(f\"Paper {row[0]} could not be deleted.\")\n",
    "            print(f\"Error Type: {type(e).__name__}\")\n",
    "            error_message = str(e)[:100]\n",
    "            print(f\"Error Message: {error_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53cb565",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get list of unprocessed papers\n",
    "list_unprocessed_papers = get_unprocessed_papers()\n",
    "print(f\"Number of unprocessed papers: {len(list_unprocessed_papers)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a24365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set how many processed papers should be processed\n",
    "number_of_papers_to_process = 5\n",
    "\n",
    "# Start the processing process, including extraction\n",
    "extract_data_from_papers(list_unprocessed_papers, number_of_papers_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c678e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
