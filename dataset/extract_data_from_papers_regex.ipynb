{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9327b72d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:16:12.592147Z",
     "start_time": "2024-10-30T18:16:12.588652Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import uuid\n",
    "import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f98dc722",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:16:15.013821Z",
     "start_time": "2024-10-30T18:16:15.009440Z"
    }
   },
   "outputs": [],
   "source": [
    "# Variables\n",
    "source_dir = \"source_files/\"\n",
    "\n",
    "table_output_dir = \"extracted_tables/\"\n",
    "table_code_dir = table_output_dir + \"table_code/\"\n",
    "table_header_dir = table_output_dir + \"table_header/\"\n",
    "table_result_file = \"tables_regex.csv\"\n",
    "\n",
    "figure_output_dir = \"extracted_figures/\"\n",
    "figure_result_file = \"figures_regex.csv\"\n",
    "\n",
    "possible_extensions = [\".pdf\", \".png\", \".jpg\", \".jpeg\", \".eps\"]\n",
    "\n",
    "os.makedirs(table_output_dir, exist_ok=True)\n",
    "os.makedirs(table_header_dir, exist_ok=True)\n",
    "os.makedirs(table_code_dir, exist_ok=True)\n",
    "os.makedirs(figure_output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bf3937e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:16:17.037975Z",
     "start_time": "2024-10-30T18:16:16.989352Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reset directories\n",
    "files = glob.glob(table_code_dir + \"*\")\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "    \n",
    "files = glob.glob(table_header_dir + \"*\")\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "    \n",
    "files = glob.glob(figure_output_dir + \"*\")\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "if os.path.isfile(table_output_dir + table_result_file):\n",
    "    os.remove(table_output_dir + table_result_file)\n",
    "    \n",
    "if os.path.isfile(figure_output_dir + figure_result_file):\n",
    "    os.remove(figure_output_dir + figure_result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9e88978",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:23:10.605702500Z",
     "start_time": "2024-10-30T18:16:19.958294Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "2001.00116v2\n",
      "tab:fpr_tpr\n",
      "can help discriminate an AE that is across the decision boundary. In detail, we use an optimal threshold based on the ROC (receiver operating characteristic) curve, to split AEs and benign images distributions. Table~\\ref{tab:fpr_tpr} presents the FPR and TPR (i.e., Detection Rate defined in Section~\\ref{sec:eval}). \\zedit{Note that the results are only for illustrating that E\\&R imposes different impacts\n",
      "tab:cw\n",
      "As shown in Table~\\ref{tab:cw}, the proposed technique achieves very high detection rates (up to 100\\% on CIFAR-10, and 99.3\\% on ImageNet) \n",
      "For another leading $L_2$ AE generation algorithm---DeepFool (see Section~\\ref{sec:deepfool}), we observe very similar results as CW-$L_2$. Table~\\ref{tab:cw} shows that our detector achieves very high detection rates (up to 99.8\\% on CIFAR-10, and 95.0\\% on ImageNet) with low FPR values. \n",
      "\\fedit{For CW-$L_2$ attack, their experiments only examine $\\kappa$ = 0.0, which is the default setting, so we also list the results under $\\kappa$ = 0.0 in Table~\\ref{tab:comp} (see Table~\\ref{tab:cw} for the results of our detector under other $\\kappa$ values). We take NIC as an example here. With respect to CIFAR-10, NIC obtains the detection rate \n",
      "%To compare with the results in Table~\\ref{tab:cw}, all the AEs are generated using the CW-$L_2$ algorithm as well. \n",
      "We adopt the SVM-based detector that achieves a detection rate of 100\\%  (Table~\\ref{tab:cw}): no AEs can fool it \\emph{without adaptive attacks}. \n",
      "tab:comp\n",
      "As summarized in Table~\\ref{tab:comp},\n",
      "\\fedit{For CW-$L_2$ attack, their experiments only examine $\\kappa$ = 0.0, which is the default setting, so we also list the results under $\\kappa$ = 0.0 in Table~\\ref{tab:comp} (see Table~\\ref{tab:cw} for the results of our detector under other $\\kappa$ values). We take NIC as an example here. With respect to CIFAR-10, NIC obtains the detection rate \n",
      "tab:cw_2\n",
      "As Table~\\ref{tab:cw_2} shows, the detection rate is as high as 100\\%. We then \n",
      "tab:l0\n",
      "Table~\\ref{tab:l0} shows the performance of this hybrid detector.\n",
      "tab:par_cifar\n",
      "increases and FPR decreases (see Table~\\ref{tab:par_cifar} and Table~\\ref{tab:par_imgNet} for more details). The reason is that by increasing $n$,\n",
      "%The results are shown in Table~\\ref{tab:par_cifar}.\n",
      "tab:par_imgNet\n",
      "increases and FPR decreases (see Table~\\ref{tab:par_cifar} and Table~\\ref{tab:par_imgNet} for more details). The reason is that by increasing $n$,\n",
      "tab:par_cifar\n",
      "increases and FPR decreases (see Table~\\ref{tab:par_cifar} and Table~\\ref{tab:par_imgNet} for more details). The reason is that by increasing $n$,\n",
      "%The results are shown in Table~\\ref{tab:par_cifar}.\n",
      "8 tables found.\n",
      "9 figures found.\n",
      "\n",
      "\n",
      "2001.00117v1\n",
      "0 tables found.\n",
      "9 figures found.\n",
      "\n",
      "\n",
      "2001.00119v2\n",
      "Document header could not be identified.\n",
      "\n",
      "\n",
      "2001.00120v1\n",
      "0 tables found.\n",
      "0 figures found.\n",
      "\n",
      "\n",
      "2001.00122v1\n",
      "tableev\n",
      "Based on this criterion, the best values we found for this pair of gambles are shown in Table \\ref{tableev}. Hence we reproduce the result that most people choose gamble A and shy away from gamble B. According to the relative signs of the $\\beta$ terms in both gambles,  a DM who chooses gamble A over gamble B is entropy seeking in gamble A but entropy averse for entropy B. From an information theory point of view, this corresponds to increasing information about A while decreasing (or erasing) information about B \\citep{Parrondo2015}. For both gambles,  a decrease in variance is energetically more favorable. This latter observation is always true in this analysis and follows from the sign of $\\lambda/\\beta$ which is always negative. A negative 'utility' can be interpreted that from an energy viewpoint, the cost of the processing of the information content exceeds that which is needed to maintain the state itself. However, it is more natural to interpret the difference as the direction of motivational momentum to drift toward one state and away from the other. The $\\mathcal{C}$ value by itself is meaningless in this model, only differences are physical.\n",
      "tableallais\n",
      "For the Allais paradox example in Fig. \\ref{allaisp1}, the values we found that fits best the physical picture advocated here are displayed in Table \\ref{tableallais}. The q-values found here give probabilities that have the usual inverse 'S'-shape as in Prospect Theory. \n",
      "For this pair of lotteries, \\citet{Birnbaum2008} gave $76\\%$ as the percentage of people who chose lottery $D$ over lottery $C$. This pair of gambles were not part of the 12 studies we quoted above. To get a percentage close to the measured value, the population must have a bias toward the $\\$2M$ state, i.e., $\\epsilon > 0.5$, and for average interaction periods between the WM and the network much shorter than the transition times between the outcomes of the $D$ gamble. The requirement of bias is not necessarily a weakness of the model, but is considered a prediction in this instance. Diffusion-based models  \\citep{Ratcliff2016}, for-example, require a bias parameter that needs to be varied to achieve agreements in two-choice decision tasks. From Table \\ref{tableallais}, we see that the focus of attention, i.e., $1/k_D$ is much longer than that of gamble $C$. Therefore, if the interaction period between memory and network is much larger than the average transition times between outcomes of gamble $C$, but less than that of gamble $D$, we can attain probabilities in the range measured by the experiment. These are details that can be checked in future measurements.  \n",
      "tablestoc\n",
      "The parameters derived for the $I-J$ pair are given in Table \\ref{tablestoc}. Again using our analogy with energy, the relative costs associated with the information content  of gamble I are much more than that of gamble J. This is reflected in the negative sign of $\\mathcal{C}$. According to our model, the reason people prefer J to I has to do more with the cost of information associated with the gamble rather than its utility, or differently, people are less motivated to drift to gamble I. Since in our model we are assuming a linear relationship between utility of outcomes and energy, the opposite of $\\mathcal{C}$, $-\\mathcal{C}$, should be interpreted from the point of view of the rest of the neuronal network as the excess energy needed to maintain the encoding of the gamble in the brain. It is this excess of energy that is minimized by the (computational part of the) brain. So it is important to remember that in classic descriptive theories, the utility is synonymous with the DM, but here utility is just the energy needed to represent the gamble in one part of the cortex. \n",
      "tableUCI\n",
      "The parameters that are consistent with a violation of UCL are shown in Table \\ref{tableUCI}. For each pair of gambles, the q-value corresponds to the maximum difference in utilities and are in the expected preference order as chosen by people. In comparing gambles $R'$ and $S'$, the difference is \n",
      "This functional dependence is plotted in Fig. \\ref{graphuc1} for different values of the initial distribution of the bits 0 and 1, and how fast memory manipulation is being carried out. Clearly, in this case for $\\epsilon = 0.5$, the attention scheme does not work since it can't decide on its own which state to choose. A slight bias is needed for a choice to be made.  According to the measurements of \\citet{Birnbaum2004b} averaged over his 12 studies, $69\\%$ of people chose gamble $R'$ over gamble $S'$. If we adopt the view that every interaction interval between the demon and the gambles are independent and correspond to different people, the probability of outgoing bit 1 should be closely aligned with the percentage of people choosing $R'$ in the limit of an infinite sample of people. Therefore, according to Eq. \\ref{pr1}, to achieve comparable percentages, the population should have an initial bias toward the higher value state of $\\$ 98$ in lottery $R'$. As an example, if we take $k_1/k_2 \\approx \\beta_{R'}/\\beta_{S'} \\approx 5$, Table \\ref{tableUCI}, and choose the rate of information manipulation by the WM to be approximately $\\omega/k_1 =2/5$, i.e., midway between the intra-transition rates, we find $p_{out}(1) \\equiv 0.63$.   \n",
      "tableUCI2\n",
      "The second pair of gambles discussed by Birnbaum is shown in Fig. \\ref{uppercum2}. The parameters corresponding to this pair of gambles are given in Table \\ref{tableUCI2}. According to the experimental measurements, about $52\\%$ of people chose $R'$ over $S'$, while about $71\\%$ of people chose $S'''$ over $R'''$, which is a violation of UCI. The much higher percentage in the $S''' > R'''$ is reflected in the magnitude of differences of entropic utilities,\n",
      "The pair of gambles are shown in Fig. \\ref{lowercum2} with the corresponding parameters in Table \\ref{tableLCI2}. According to the measurements, about $51\\%$ of people prefer $S$ over $R$, while about $65\\%$ prefer $R''$ over $S''$ which is a violation of LCI.  The parameters determined from our theory for this pair of gambles are given in Table \\ref{tableUCI2}. The effective motivational effort, $\\mathcal{C}$,  is larger for the $R''S''$ pair than for the $RS$ pair,  $\\Delta \\mathcal{C}_{SR} =  0.53 \\,\\, <  \\,\\,  \\Delta \\mathcal{C}_{R\"S\"} = 0.92$. This is indirectly reflected in the percentage numbers quoted above, which imply that it is cognitively easier for the process $R'' > S''$ to be processed than that of $S > R$. Note that according to our theory, we cannot conclude for certain that $\\mathcal{C}_{R\"S} = 6.96$ is larger than $\\mathcal{C}_{S''R} = 6.54$ since all the parameters will be different, but what we can conclude is that the reaction time to choose $S$ should be slower than that of choosing $R''$.  Only a fully dynamical theory, with higher-order terms in values, can account for these differences in transition rates. In the next section, we propose some ideas on how to approach this problem using the Maxwell 'demon' analogy \\citep{Parrondo2015}.\n",
      " The plot is shown for the case when $k_{S'''}/k_{R'''} \\approx 0$ , which is implied by entropic utility, Table \\ref{tableUCI2}. This result is physically plausible. The parameters $\\omega$ and $\\epsilon$ reflect cognitive effort by the WM, this implies that the memory can interact very little with the network and induce a decision. Therefore, compared to the decision made on the $R'S'$ pair, it is  easier to make a decision on this pair. Again, this is very reasonable given the complexity of the first pair. Since the measured probabilities of the first decision are higher than this current one, this may lead us to believe that it is easier to make a decision on the first pair rather than this one. From the structure of both pairs, it is very difficult to agree with this interpretation, and we believe that the conclusion from this model is more plausible in this case.\n",
      "tableLCI2\n",
      "The pair of gambles are shown in Fig. \\ref{lowercum2} with the corresponding parameters in Table \\ref{tableLCI2}. According to the measurements, about $51\\%$ of people prefer $S$ over $R$, while about $65\\%$ prefer $R''$ over $S''$ which is a violation of LCI.  The parameters determined from our theory for this pair of gambles are given in Table \\ref{tableUCI2}. The effective motivational effort, $\\mathcal{C}$,  is larger for the $R''S''$ pair than for the $RS$ pair,  $\\Delta \\mathcal{C}_{SR} =  0.53 \\,\\, <  \\,\\,  \\Delta \\mathcal{C}_{R\"S\"} = 0.92$. This is indirectly reflected in the percentage numbers quoted above, which imply that it is cognitively easier for the process $R'' > S''$ to be processed than that of $S > R$. Note that according to our theory, we cannot conclude for certain that $\\mathcal{C}_{R\"S} = 6.96$ is larger than $\\mathcal{C}_{S''R} = 6.54$ since all the parameters will be different, but what we can conclude is that the reaction time to choose $S$ should be slower than that of choosing $R''$.  Only a fully dynamical theory, with higher-order terms in values, can account for these differences in transition rates. In the next section, we propose some ideas on how to approach this problem using the Maxwell 'demon' analogy \\citep{Parrondo2015}.\n",
      "6 tables found.\n",
      "19 figures found.\n",
      "\n",
      "\n",
      "2001.00124v1\n",
      "0 tables found.\n",
      "2 figures found.\n",
      "\n",
      "\n",
      "2001.00125v1\n",
      "0 tables found.\n",
      "0 figures found.\n",
      "\n",
      "\n",
      "2001.00126v1\n",
      "6 tables found.\n",
      "0 figures found.\n",
      "\n",
      "\n",
      "2001.00127v2\n",
      "File not found: 3-b.pdf - [Errno 2] No such file or directory: '3-b.pdf'\n",
      "File not found: 3-c.pdf - [Errno 2] No such file or directory: '3-c.pdf'\n",
      "File not found: 3-d.pdf - [Errno 2] No such file or directory: '3-d.pdf'\n",
      "File not found: m6-2.pdf - [Errno 2] No such file or directory: 'm6-2.pdf'\n",
      "File not found: m6-3.pdf - [Errno 2] No such file or directory: 'm6-3.pdf'\n",
      "File not found: m6-4.pdf - [Errno 2] No such file or directory: 'm6-4.pdf'\n",
      "0 tables found.\n",
      "6 figures found.\n",
      "\n",
      "\n",
      "2001.00128v3\n",
      "tab:1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So far, the anomaly in all types of the local linear transformation of fermion fields\\footnote{The local and linear transformation of $\\psi(x),\\bar{\\psi}(x)$ must be $\\delta\\psi(x)=\\alpha(x)\\Omega\\psi(x),\\delta\\bar{\\psi}(x)=\\alpha(x)\\bar{\\psi}(x)\\widetilde{\\Omega}$, where $\\Omega$ and $\\widetilde{\\Omega}$ are a linear combination of $\\gamma$ matrices and hence of $\\mathbbm{1},\\gamma^\\mu,[\\gamma^\\mu,\\gamma^\\nu],\\gamma^\\mu\\gamma_5,\\gamma_5$. However, the traces of odd number $\\gamma$ matrices are zero; i.e., $\\mathrm{tr}[\\gamma^\\mu]\\delta^4(x-x)$ and $\\mathrm{tr}[\\gamma^\\mu,\\gamma_5]\\delta^4(x-x)$ are zero even after regularization.} (not all symmetry transformations) has been exhausted. There are only three non-zero anomalies; see Table.\\ref{tab:1} (in Fujikawa's style for simplicity).\n",
      "Table.\\ref{tab:1} shows that the transverse anomalies have many more types of operators than the trace anomaly and chiral anomaly. In particular, the $C_{abc}F_b^{\\mu\\rho}F_{c\\rho}^{\\ \\ \\nu}$ term may have some effect on the present scheme\\cite{qin2013,qin2014,albino2019} making use of tWTI. However, in this scheme, the other two terms in non-Abelian transverse anomalies and the whole Abelian transverse anomalies (where $C_{abc}=0$) have no places to plug in, because the general method\\cite{qin2013,qin2014} is to contract $\\epsilon_{\\alpha\\mu\\nu\\beta}t_\\alpha q_\\beta,\\epsilon_{\\alpha\\mu\\nu\\beta}\\gamma_\\alpha q_\\beta$ to the vector tWTI\\footnote{So far, \\cite{qin2013,qin2014} discussed only the Abelian case. And here we use the Abelian tWTI for an explanation.} in the momentum space\\footnote{Eq.(4) in\\cite{qin2013}, in the Euclidean metric; $q\\equiv k-p,t\\equiv k+p$.} :\n",
      "1 tables found.\n",
      "2 figures found.\n",
      "\n",
      "\n",
      "2001.00131v1\n",
      "case_table\n",
      "\\caption{Three-dimensional Shannon Mapping geometry to assist the distribution case partitioning in Table~\\ref{case_table}. Only case b1 and its Left and Right Stage Crossings are shown. Other cases can be similarly visualized.}\\label{fig:3-dimen-shannon-map}\n",
      "Based on geometry, as observed from Fig.~\\ref{fig:3-dimen-shannon-map}, we partition the stage crossing into three major cases---\\textit{a)}, \\textit{b)}, and \\textit{c)}---based on the locations of the transmitted points. In case \\textit{a)}, the point is located at any stage besides top and bottom stage of any z-plane. In case \\textit{b)}, the point is located at the top or bottom stages of z-plane besides the first and last z-plane. In case \\textit{c)}, the point is located at the top or bottom stage of the first and last z-plane. In each case, the distribution differs for odd and even number of stages and z-planes, and differs for LSC and RSC and other geometry-specific information. The values of variables $\\nu_x$, $\\nu_y$, and $\\nu_z$ for different cases and sub-cases are listed in Table~\\ref{case_table} along with corresponding parameter conditions.\n",
      "In case group b), point $(x_r, y_r, z_r)$ is located at the top or bottom stage, but not at the 1st or last z-plane. There are four sub-cases of b1) to b4) explained in Table~\\ref{case_table}. In case group c), point $(x_r, y_r, z_r)$ is located at the 1st or last z-plane. The four sub-cases of c1) to c4) are also explained in Table~\\ref{case_table}. Readers can refer to the table for details of these sub-cases. \n",
      "File not found: fig/System_Model.pdf - [Errno 2] No such file or directory: 'fig/System_Model.pdf'\n",
      "1 tables found.\n",
      "8 figures found.\n",
      "\n",
      "\n",
      "2001.00132v1\n",
      "tab:notations\n",
      "tab:dataset_stats\n",
      "Dataset statistics are provided in Table~\\ref{tab:dataset_stats}.\n",
      "tab:map_results\n",
      "We note the following key observations from our experimental results comparing~\\name~against competing baselines (Table~\\ref{tab:map_results}).\n",
      "tab:ablation_results\n",
      "4 tables found.\n",
      "6 figures found.\n",
      "\n",
      "\n",
      "2001.00133v2\n",
      "table:summary\n",
      "to the value of $\\rho_0$ in Table~\\ref{table:summary}.  The initial conditions are \n",
      "In Table~\\ref{table:summary}, we list model parameters and summarize gross outflow properties. \n",
      "to grow.  In terms of timescales, the flow must satisfy $t_{\\rm{cool}} < t_{\\rm{sc}}$ on global scales, which it does (see Table~\\ref{table:summary}).  However, Fig.~\\ref{fig:1d-summary-phase} shows that for the flow to remain in the TI-zone, it must also be the case that $\\Xi$ \\textit{not} increase downstream, the normal tendency in disk winds, at least in the absence of magnetic field pressure \\citep{HP15}. \n",
      "File not found: figure-01a-08.pdf - [Errno 2] No such file or directory: 'figure-01a-08.pdf'\n",
      "19 tables found.\n",
      "5 figures found.\n",
      "\n",
      "\n",
      "2001.00134v1\n",
      "0 tables found.\n",
      "0 figures found.\n",
      "\n",
      "\n",
      "2001.00136v1\n",
      "0 tables found.\n",
      "1 figures found.\n",
      "\n",
      "\n",
      "2001.00137v2\n",
      "tbl:twitter_mistakes\n",
      "In order to evaluate the performance of our model, we need access to a naturally noisy dataset with real human errors. Poor quality texts obtained from Twitter, called tweets, are then ideal for our task. For this reason, we choose Kaggle's two-class Sentiment140 dataset~\\cite{go2009twitterSentiment140}\\footnote{\\url{https://www.kaggle.com/kazanova/sentiment140}}, which consists of spoken text being used in writing and without strong consideration for grammar or sentence correctness. Thus, it has many mistakes, as specified in Table \\ref{tbl:twitter_mistakes}~\\cite{lourentzou2019adapting}.\n",
      "tbl:twitter_examples\n",
      "Even though this corpus has incorrect sentences and their emotional labels, they lack their respective corrected sentences, necessary for the training of our model. In order to obtain this missing information, we use Amazon Mechanical Turk (MTurk)~\\cite{buhrmester2011amazon}, a paid marketplace for Human Intelligence Tasks (HITs) which allows for anonymity between ``requesters” and ``workers”. This ensures that the requester is not able to influence the worker into answering a survey the way they want, reducing bias and allowing for believable results to be obtained. In this work, we simply use MTurk to outsource native English speakers to obtain the correct sentences from the original tweets. More specifically, human annotators are given a list of original tweets and they are asked to correct them with as little change as possible and without inserting any extra punctuation marks unless absolutely necessary, following guidelines for noisy text normalization~\\cite{lourentzou2019adapting}. After getting the data back, we manually check if the corrections are acceptable, otherwise we post another HIT. We claim this is unbiased because this is only dependent on the English language, not on the sentiment or corrector's background. Some examples are shown in Table \\ref{tbl:twitter_examples}.\n",
      "tbl:corpus_info_twitter\n",
      "After obtaining the correct sentences, our two-class dataset~\\footnote{Available at \\url{https://github.com/gcunhase/StackedDeBERT}} has class distribution as shown in Table \\ref{tbl:corpus_info_twitter}. There are 200 sentences used in the training stage, with 100 belonging to the \\textit{positive} sentiment class and 100 to the \\textit{negative} class, and 50 samples being used in the evaluation stage, with 25 negative and 25 positive. This totals in 250 samples, with incorrect and correct sentences combined. Since our goal is to evaluate the model's performance and robustness in the presence of noise, we only consider incorrect data in the testing phase. Note that BERT is a pre-trained model, meaning that small amounts of data are enough for appropriate fine-tuning.\n",
      "Another interesting observation in the Twitter dataset is that our proposed model more significantly improves the performance of class 1 (positive) with a small decrease of performance of class 0 (negative). However, the same pattern is not observed when we compare performance in the Chatbot dataset, where the proposed model shows minor improvements in class 0 (Departure Time intent) while maintaining the performance of class 1 (Find Connection intent). We believe that the in-class improvement of class 1 noticed in the Twitter dataset is due to the existence of a larger gap in performance between classes 0 and 1 in the baseline BERT model, which allows for our model to achieve better overall accuracy with a trade-off of performances. After an investigation of possible reasons why that gap in performance exists in the Twitter dataset, we conclude that that is due to differing average number of words in sentences from positive/negative classes and from train/test sets. Table~\\ref{tbl:average_number_of_words_per_sentence} shows a comparison between the average number of words in the Twitter dataset and both TTS-STT Chatbot copora. As can be seen, the Chatbot corpus consistently maintains an average number of 9 to 10 words per sentence for both classes in both the train and test sets. Whereas the Twitter dataset has an average of 16 words for both classes during training and an average of 24 words for the negative class versus 9 words for the positive during testing, which explains the better performance in the positive class. A larger test set might also be influencing the smaller performance improvement detected in the Chatbot corpus (see Tables~\\ref{tbl:corpus_info_twitter} and~\\ref{tbl:corpus_info}).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tbl:corpus_info\n",
      "The dataset used to evaluate the models' performance is the Chatbot Natural Language Understanding (NLU) Evaluation Corpus, introduced by Braun et al.~\\cite{braun2017evaluating} to test NLU services. It is a publicly available~\\footnote{\\url{https://github.com/sebischair/NLU-Evaluation-Corpora}} benchmark and is composed of sentences obtained from a German Telegram chatbot used to answer questions about public transport connections. The dataset has two intents, namely \\textit{Departure Time} and \\textit{Find Connection} with 100 train and 106 test samples, shown in Table \\ref{tbl:corpus_info}. Even though English is the main language of the benchmark, this dataset contains a few German station and street names.\n",
      "Another interesting observation in the Twitter dataset is that our proposed model more significantly improves the performance of class 1 (positive) with a small decrease of performance of class 0 (negative). However, the same pattern is not observed when we compare performance in the Chatbot dataset, where the proposed model shows minor improvements in class 0 (Departure Time intent) while maintaining the performance of class 1 (Find Connection intent). We believe that the in-class improvement of class 1 noticed in the Twitter dataset is due to the existence of a larger gap in performance between classes 0 and 1 in the baseline BERT model, which allows for our model to achieve better overall accuracy with a trade-off of performances. After an investigation of possible reasons why that gap in performance exists in the Twitter dataset, we conclude that that is due to differing average number of words in sentences from positive/negative classes and from train/test sets. Table~\\ref{tbl:average_number_of_words_per_sentence} shows a comparison between the average number of words in the Twitter dataset and both TTS-STT Chatbot copora. As can be seen, the Chatbot corpus consistently maintains an average number of 9 to 10 words per sentence for both classes in both the train and test sets. Whereas the Twitter dataset has an average of 16 words for both classes during training and an average of 24 words for the negative class versus 9 words for the positive during testing, which explains the better performance in the positive class. A larger test set might also be influencing the smaller performance improvement detected in the Chatbot corpus (see Tables~\\ref{tbl:corpus_info_twitter} and~\\ref{tbl:corpus_info}).\n",
      "tbl:missing_data\n",
      "Table \\ref{tbl:missing_data} exemplifies a complete and its respective incomplete sentences with different TTS-STT combinations, thus varying rates of missing and incorrect words. The level of noise is denoted by two metrics: inverted BLEU (iBLEU) and Word Error Rate (WER) score. The inverted BLEU score ranges from $0$ to $1$ and is denoted by Eq.~\\eqref{eq:ibleu}:\n",
      "tbl:twitter_f1_scores\n",
      "Experimental results for the Twitter Sentiment Classification task on Kaggle's Sentiment140 Corpus dataset, displayed in Table \\ref{tbl:twitter_f1_scores}, show that our model has better F1-micros scores, outperforming the baseline models by 6$\\%$ to 8$\\%$. We evaluate our model and baseline models on three versions of the dataset. The first one (\\textit{Inc}) only considers the original data, containing naturally incorrect tweets, and achieves accuracy of 80$\\%$ against BERT's 72$\\%$. The second version (\\textit{Corr}) considers the corrected tweets, and shows higher accuracy given that it is less noisy. In that version, Stacked DeBERT achieves 82$\\%$ accuracy against BERT's 76$\\%$, an improvement of 6$\\%$. In the last case (\\textit{Inc+Corr}), we consider both incorrect and correct tweets as input to the models in hopes of improving performance. However, the accuracy was similar to the first aforementioned version, 80$\\%$ for our model and 74$\\%$ for the second highest performing model. Since the first and last corpus gave similar performances with our model, we conclude that the Twitter dataset does not require complete sentences to be given as training input, in addition to the original naturally incorrect tweets, in order to better model the noisy sentences.\n",
      "In addition to the overall F1-score, we also present a confusion matrix, in Fig. \\ref{fig:twitter_confusion_matrix_plot}, with the per-class F1-scores for BERT and Stacked DeBERT. The normalized confusion matrix plots the predicted labels versus the target/target labels. Similarly to Table \\ref{tbl:twitter_f1_scores}, we evaluate our model with the original Twitter dataset, the corrected version and both original and corrected tweets. It can be seen that our model is able to improve the overall performance by improving the accuracy of the lower performing classes. In the \\textit{Inc} dataset, the true class 1 in BERT performs with approximately 50\\%. However, Stacked DeBERT is able to improve that to 72\\%, although to a cost of a small decrease in performance of class 0. A similar situation happens in the remaining two datasets, with improved accuracy in class 0 from 64\\% to 84\\% and 60\\% to 76\\% respectively.\n",
      "where $P$ is precision, $R$ is recall, and $TP$, $TN$, $FP$, and $FN$ are as indicated in Fig~\\ref{fig:confusion_matrix_explanation}. Since the micro-averaged scores are the same, we simply display the micro-F1 score as per usual in the literature. We also further analyze the confusion matrices obtained from both datasets in regards to Type I and II errors. In the Twitter sentiment classification task (Fig.~\\ref{tbl:twitter_f1_scores}), our model achieves better overall performance by trading-off accuracy with the best performing class 0, meaning that our model performs slightly worse in the TN and Type I error and significantly better in the Type II and TP errors when compared to the baseline model. In the Chatbot intent classification task (Fig.~\\ref{tbl:chatbot_missing_words_f1}), our model performs better or similarly in all instances.\n",
      "tbl:chatbot_missing_words_f1\n",
      "Experimental results for the Intent Classification task on the Chatbot NLU Corpus with STT error can be seen in Table \\ref{tbl:chatbot_missing_words_f1}. When presented with data containing STT error, our model outperforms all baseline models in both combinations of TTS-STT: \\textit{gtts-witai} outperforms the second placing baseline model by 0.94\\% with F1-score of 97.17\\%, and \\textit{macsay-witai} outperforms the next highest achieving model by 1.89\\% with F1-score of 96.23\\%.\n",
      "Here we analyze the robustness of our model given varying levels of noise in the Chatbot Intent Classification Corpora. Table~\\ref{tbl:chatbot_missing_words_f1} indicates the level of noise in each TTS-STT noisy dataset with their respective iBLEU and WER scores, where 0 means no noise and higher values mean higher quantity of noise. As expected, the models' accuracy degrade with the increase in noise, thus F1-scores of \\textit{gtts-witai} are higher than \\textit{macsay-witai}. However, our model does not only outperform the baseline models but does so with a wider margin. This is shown with the increasing robustness plot in Fig.~\\ref{fig:chatbot_stterror_robustness_curve} and can be demonstrated by \\textit{macsay-witai} outperforming the baseline models by twice the gap achieved by \\textit{gtts-witai}.\n",
      "Further analysis of the results in Table \\ref{tbl:chatbot_missing_words_f1} show that, BERT decay is almost constant with the addition of noise, with the difference between the complete data and \\textit{gtts-witai} being 1.88 and \\textit{gtts-witai} and \\textit{macsay-witai} being 1.89. Whereas in Stacked DeBERT, that difference is 1.89 and 0.94 respectively. This is stronger indication of our model's robustness in the presence of noise.\n",
      "where $P$ is precision, $R$ is recall, and $TP$, $TN$, $FP$, and $FN$ are as indicated in Fig~\\ref{fig:confusion_matrix_explanation}. Since the micro-averaged scores are the same, we simply display the micro-F1 score as per usual in the literature. We also further analyze the confusion matrices obtained from both datasets in regards to Type I and II errors. In the Twitter sentiment classification task (Fig.~\\ref{tbl:twitter_f1_scores}), our model achieves better overall performance by trading-off accuracy with the best performing class 0, meaning that our model performs slightly worse in the TN and Type I error and significantly better in the Type II and TP errors when compared to the baseline model. In the Chatbot intent classification task (Fig.~\\ref{tbl:chatbot_missing_words_f1}), our model performs better or similarly in all instances.\n",
      "tbl:macro_results_twitter\n",
      "We calculate the macro-average precision, recall, and F1 scores with each dataset and show that our model outperforms the baseline in all cases. Note that, following the same pattern as the micro-F1 scores, our model significantly outperforms the baseline model in the Twitter sentiment classification task and it outperforms the baseline model in the Chatbot intent classification task with a smaller margin. These results are shown in Tables~\\ref{tbl:macro_results_twitter} and~\\ref{tbl:macro_results_chatbot}.\n",
      "tbl:macro_results_chatbot\n",
      "We calculate the macro-average precision, recall, and F1 scores with each dataset and show that our model outperforms the baseline in all cases. Note that, following the same pattern as the micro-F1 scores, our model significantly outperforms the baseline model in the Twitter sentiment classification task and it outperforms the baseline model in the Chatbot intent classification task with a smaller margin. These results are shown in Tables~\\ref{tbl:macro_results_twitter} and~\\ref{tbl:macro_results_chatbot}.\n",
      "tbl:noise_level_comparison_performance\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When comparing the improvement in performance in the Twitter and TTS-STT Chatbot datasets, we notice that the former shows major improvements whilst the latter shows only minor improvements. We investigate if this is due to lower noise levels in the Twitter dataset. However, as can be seen in Table~\\ref{tbl:noise_level_comparison_performance}, our model’s better performance in the Twitter dataset is not related to it having lower noise levels when compared to the TTS-STT Chatbot corpus. A possible reason as to why our model is able to improve its performance by a larger margin in the Twitter dataset can be due to BERT being trained on the Wikipedia and Book Corpus~\\cite{devlin2018bert}. Twitter has arguably more noise when compared to BERT’s original training data due to its highly informal setting and character limitation. However, studies suggest that social media text has relatively small grammatical disparity when compared to edited text such as Wikipedia~\\cite{baldwin2013noisy}, and since they both contain user generated texts, their basic sentence structure is still much more similar than when compared to sentences with STT error. This different word composition in sentences affected with STT error, makes it harder for the model to perform as well as its counterpart trained with user-generated text.\n",
      "tbl:average_number_of_words_per_sentence\n",
      "Another interesting observation in the Twitter dataset is that our proposed model more significantly improves the performance of class 1 (positive) with a small decrease of performance of class 0 (negative). However, the same pattern is not observed when we compare performance in the Chatbot dataset, where the proposed model shows minor improvements in class 0 (Departure Time intent) while maintaining the performance of class 1 (Find Connection intent). We believe that the in-class improvement of class 1 noticed in the Twitter dataset is due to the existence of a larger gap in performance between classes 0 and 1 in the baseline BERT model, which allows for our model to achieve better overall accuracy with a trade-off of performances. After an investigation of possible reasons why that gap in performance exists in the Twitter dataset, we conclude that that is due to differing average number of words in sentences from positive/negative classes and from train/test sets. Table~\\ref{tbl:average_number_of_words_per_sentence} shows a comparison between the average number of words in the Twitter dataset and both TTS-STT Chatbot copora. As can be seen, the Chatbot corpus consistently maintains an average number of 9 to 10 words per sentence for both classes in both the train and test sets. Whereas the Twitter dataset has an average of 16 words for both classes during training and an average of 24 words for the negative class versus 9 words for the positive during testing, which explains the better performance in the positive class. A larger test set might also be influencing the smaller performance improvement detected in the Chatbot corpus (see Tables~\\ref{tbl:corpus_info_twitter} and~\\ref{tbl:corpus_info}).\n",
      "File not found: stacked_debert_compact.pdf - [Errno 2] No such file or directory: 'stacked_debert_compact.pdf'\n",
      "11 tables found.\n",
      "6 figures found.\n",
      "\n",
      "\n",
      "2001.00138v4\n",
      "0 tables found.\n",
      "0 figures found.\n",
      "\n",
      "\n",
      "2001.00139v1\n",
      "Document header could not be identified.\n",
      "\n",
      "\n",
      "2001.00140v3\n",
      "0 tables found.\n",
      "22 figures found.\n",
      "\n",
      "\n",
      "2001.00141v1\n",
      "File not found: fig_vector.pdf - [Errno 2] No such file or directory: 'fig_vector.pdf'\n",
      "0 tables found.\n",
      "4 figures found.\n",
      "\n",
      "\n",
      "2001.00143v3\n",
      "tab:case1\n",
      "In the first numerical case, we have 5 observations, as listed in Table~\\ref{tab:case1}. There are two known constraints with the first one being the half-space  $\\cC$, as discussed in Section~\\ref{sec:Methodology}.  \n",
      "tab:case2\n",
      "In this section, we test our approach on a relatively larger numerical example. This example considers 19 observations with $\\bx^0 = (1,1)$, two known constraints (the first one being the half-space $\\cC$), and 6 unknown constraints. The details of this numerical example are summarized in Table~\\ref{tab:case2}. Given the larger size of this example, the nonlinear solver {MINOS} was not able to find the global optimal solutions for most instances of the problem. Hence, we were only able to solve this example using the $\\eMIO$ formulation, which further illustrates the importance and advantage of the proposed $\\eMIO$ formulation in solving larger instances to optimality. This advantage is more pronounced when the normalization constraint~\\eqref{eq:MIPnorm} is linear and the $\\eMIO$ problem becomes a linearly-constrained optimization model. \n",
      "tab:case3\n",
      "We tested the proposed $\\MIO$ model with two different known objective functions: ({\\it a}\\,) maximizing daily protein intake and ({\\it b}\\,) minimizing daily sodium intake. For each objective, we found the preferred observation $\\bx^0$ (\\ie, the observation with the best objective value) and recovered a feasible region that made all 100 observations feasible and $\\bx^0$ optimal for the corresponding forward problem. Table~\\ref{tab:case3} shows a summary of the known constraints and the inverse problems, and Table~\\ref{tab:case3_fooddetails} in the Appendix provides additional information about the observations. \n",
      "tab:case3error\n",
      "Finally, we quantify the differences between the diets recommended with and without the imputed $\\MIO$ constraints for each objective by finding the average $L_1$~norm distance of all the observations from each of these diets as depicted in Table~\\ref{tab:case3error}. In both cases, the diet with $\\MIO$ constraints is closer to the observations. \n",
      "tab:case3_fooddetails\n",
      "We tested the proposed $\\MIO$ model with two different known objective functions: ({\\it a}\\,) maximizing daily protein intake and ({\\it b}\\,) minimizing daily sodium intake. For each objective, we found the preferred observation $\\bx^0$ (\\ie, the observation with the best objective value) and recovered a feasible region that made all 100 observations feasible and $\\bx^0$ optimal for the corresponding forward problem. Table~\\ref{tab:case3} shows a summary of the known constraints and the inverse problems, and Table~\\ref{tab:case3_fooddetails} in the Appendix provides additional information about the observations. \n",
      "Table~\\ref{tab:case3_fooddetails} shows the summary of the 100 observations (\\ie, days) of daily food intake.  The column `count' shows how many times each food was consumed (\\ie, in how many of the 100 observations). The next two columns show the average and standard deviation of the number of servings of each food, in the days that the food was consumed. \n",
      "5 tables found.\n",
      "8 figures found.\n",
      "\n",
      "\n",
      "2001.00145v1\n",
      "0 tables found.\n",
      "0 figures found.\n",
      "\n",
      "\n",
      "2001.00147v1\n",
      "File not found: cr_nocr.pdf - [Errno 2] No such file or directory: 'cr_nocr.pdf'\n",
      "File not found: inc_withcr.pdf - [Errno 2] No such file or directory: 'inc_withcr.pdf'\n",
      "0 tables found.\n",
      "6 figures found.\n",
      "\n",
      "\n",
      "2001.00148v1\n",
      "tab:obs\n",
      "    The details of each observation are listed in table~\\ref{tab:obs}.\n",
      "      Note that HOWPol data were binned with $\\sim 130\\>\\mathrm{s}$ width because their exposure time, $30\\>\\mathrm{s}$, is shorter than HONIR's exposure times, $75$--$110\\>\\mathrm{s}$ (see also table~\\ref{tab:obs}).\n",
      "tab:allparams\n",
      "        \\caption{Obtained light curves of PTFO\\,8-8695 from February 2014 to December 2016. The solid lines indicate the best-fitting light curve models and dashed lines indicate the trend functions. Gray-shaded areas indicate the expected time windows of dip-B (see subsection~\\ref{sec:ephemeris}), whose centers are calculated from equation~(\\ref{eq:ephemeris}) and widths are fixed to a minimum value, $0.026\\>\\mathrm{d}$ of the duration in table~\\ref{tab:allparams}. The lower panels of each light curve show residuals from the best-fitting models.}\\label{fig:rawlc_1}\n",
      "      The best-fitting parameters of the dip function are listed in table~\\ref{tab:allparams} and the best-fitting light curves are shown as solid lines in figures~\\ref{fig:rawlc_1} and~\\ref{fig:rawlc_2}.\n",
      "      We fitted the center times of dip-B listed in table~\\ref{tab:allparams} by weighted least squares method.\n",
      "            Note that we set an upper limit for the inclination angle corresponding to the upper side of 95\\% credibility interval when dip-B is not detected (see also table~\\ref{tab:allparams}).\n",
      "tab:wldepend\n",
      "      All phase-folded light curves and the best-fitting fading depths are shown in figure~\\ref{fig:folded} and table~\\ref{tab:wldepend}, respectively.\n",
      "      As listed in table~\\ref{tab:wldepend}, both dip-A's and dip-C's infrared-to-optical depth ratios are about 0.7.\n",
      "        Then, fixing $T_*$ to $3470\\>\\mathrm{K}$ \\citep{Briceno2005}, we fit the obtained ratios shown in table~\\ref{tab:wldepend}.\n",
      "        After calculating $\\alpha$ for each combination of $i$ and $\\phi$, which satisfies the observed depths [i.e., 1.8\\% for the 2014 season and 1.3\\% for the 2016 season (see also table~\\ref{tab:wldepend})], we derive the duration of the dip from $\\alpha$, $i$, and $\\phi$.\n",
      "          \\caption{Contour maps of the infrared-to-optical depth ratio of dip-A in the 2014 season. \\textit{J-, H-} and $K_s$-band data are shown in the top, middle, and bottom panels, respectively. Panels on the right column are enlarged ones of those on the left column. Solid lines indicate loci of the infrared-to-optical depth ratios: 0.79 for the \\textit{J} band, 0.61 for the \\textit{H} band, and 0.63 for the $K_s$ band. Dashed lines indicate a few $\\sigma$ levels of the observed depth ratio (see also table~\\ref{tab:wldepend}). Gray areas are parameter spaces to yield 1.75\\%--2.25\\% depth at $I_C$ band.}\\label{fig:hotspot_A14}\n",
      "          As listed in table~\\ref{tab:wldepend}, $3\\,\\sigma$ ranges of infrared-to-optical depth ratios in both the 2014 and 2016 seasons do not allow unity, which would be expected for a planetary transit.\n",
      "            Then, fixing the spot temperature to $2410\\>\\mathrm{K}$, we examine whether this spot satisfies the observed depth and duration, as listed in table~\\ref{tab:wldepend}.\n",
      "            As listed in table~\\ref{tab:wldepend}, the \\textit{J-}to-$I_C$ depth ratio does not include 1 within $3\\,\\sigma$ range.\n",
      "tab:dust\n",
      "          Fixing $r$ to the best-fitting value at $R_V = 5.3$ because the fitting results do not depend on the value of $R_V$ much, we derive $f_\\mathrm{core}$ and $f_\\mathrm{halo}\\tau_V$ from the observed dip depth at $I_C$ band, as listed in table~\\ref{tab:dust}.\n",
      "tab:orb_param\n",
      "            The best-fitting parameters from $10^7\\>$MCMC steps, except the first $10^6\\>$steps, are listed in table~\\ref{tab:orb_param}.\n",
      "            Then, fixing all parameters to the values listed in table~\\ref{tab:orb_param}, except the inclination angle, we fit the inclination angle for each observational date individually.\n",
      "            One concern is that our derived parameters listed in table~\\ref{tab:orb_param} imply a smaller stellar radius, $0.45\\,\\pm\\,0.18\\,R_{\\solar}$ than that derived from the stellar luminosity in \\citet{Briceno2005}.\n",
      "            In this case, the ratio of planetary radius to stellar radius is $\\sqrt{f_\\mathrm{core}}$, 0.05, which yields a planetary radius of $0.22\\,R_\\mathrm{Jup}$ in combination with the stellar radius of $0.45\\,R_{\\solar}$ from table~\\ref{tab:orb_param}.\n",
      "            It is consistent with the inclination angle of $78.4^\\circ$ in table~\\ref{tab:orb_param} but a different light curve\n",
      "          Employing our derived transit parameters in table~\\ref{tab:orb_param}, the reflected light and thermal emission from the planet would be 0.05 and 0.26 times weaker than their estimation, respectively.\n",
      "tab:young\n",
      "          From the viewpoint of the youth, this is the third planet candidate younger than a few Myr, as listed in table~\\ref{tab:young}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 tables found.\n",
      "19 figures found.\n",
      "\n",
      "\n",
      "2001.00151v1\n",
      "0 tables found.\n",
      "5 figures found.\n",
      "\n",
      "\n",
      "2001.00152v1\n",
      "Table 1\n",
      "%If $\\lambda_n$ decays faster than $O_p(n^{-\\frac{2m}{4m+d}})$, Theorem \\ref{th:improvedrate} is not applicable. In this case, we can still invoke Corollary \\ref{Coro:rate} to obtain slower rates of convergence. We summarize the details in Table \\ref{Table 1}.\n",
      "File not found: lexample_fig1 - [Errno 2] No such file or directory: 'lexample_fig1'\n",
      "1 tables found.\n",
      "2 figures found.\n",
      "\n",
      "\n",
      "2001.00153v1\n",
      "Document header could not be identified.\n",
      "\n",
      "\n",
      "2001.00154v1\n",
      "0 tables found.\n",
      "6 figures found.\n",
      "\n",
      "\n",
      "2001.00160v1\n",
      "0 tables found.\n",
      "5 figures found.\n",
      "\n",
      "\n",
      "2001.00161v1\n",
      "tab:parametersBC\n",
      "Three sets of parameters of the charm and bottom system corresponding to the varying of the interaction width are listed in Table~\\ref{tab:parametersBC}.\n",
      "\\caption{\\label{tab:massccbar} Masses (in MeV) of the charmonium with $J^{PC}\\,=\\,0^{-+},\\,1^{--},\\,0^{++},\\,1^{+-},\\,1^{++},\\,2^{++}$, the normal states in the quark model. $M^{\\textmd{RL}}_{c\\bar{c}}$ is our RL approximation result. $M^{\\textmd{expt.}}_{c\\bar{c}}$ is the experiment value \\cite{Tanabashi2018}. $\\Delta M^{\\textmd{RL}}_{c\\bar{c}} = M^{\\textmd{RL}}_{c\\bar{c}} - M^{\\textmd{expt.}}_{c\\bar{c}}$ is the deviation of our results from the experiment value. Three sets of parameters in Tab. \\ref{tab:parametersBC} are used in our calculation. }\n",
      "tab:massccbar\n",
      "The masses of the charmonium are listed in Tab. \\ref{tab:massccbar}.\n",
      "\\caption{\\label{tab:massbbbar} Masses (in MeV) of the bottomonium. The meanings of the quantities are the same as in Tab. \\ref{tab:massccbar}.}\n",
      "The results in Tab. \\ref{tab:massccbar} and Tab. \\ref{tab:massbbbar} have two-sided meanings. On one hand, the RL approximation is reasonble for the charmonium and bottomonium system. The mass deviations are less than $3.5\\%$ for the charmonium and less than $1.1\\%$ for the bottomonium. However, the absolute errors do not decrease for the P-wave states, indicating that in the RL approximation some interactions are still lacking even for the heavy system. On the other hand, the results are stable when the parameters change. The uncertainties due to the varying of the parameters are small, less than $0.8\\%$ for the charmonium and less than $0.2\\%$ for the bottomonium. So the errors (defined by Eq.(\\ref{eq:deltaMccbar}) and Eq.(\\ref{eq:deltaMbbbar})) of the RL approximation could be estimated quantitatively.\n",
      "i.e., averaging the $C$-parity. In Tab. \\ref{tab:massccbar} and  Tab. \\ref{tab:massbbbar}, the difference of $\\Delta M^{\\textmd{RL}}_{c\\bar{c}}$ with $J^{PC}=1^{++}$ and $J^{PC}=1^{+-}$ are less than $31\\textmd{ MeV}$, and there is no difference for $\\Delta M^{\\textmd{RL}}_{b\\bar{b}}$ for these two states. So taking the $C-$parity average leads no more than $8\\textmd{ MeV}$ error for $\\Delta M^{\\textmd{RL}}_{c\\bar{b},1^+}$. \n",
      "tab:massbbbar\n",
      "The masses of the bottomonium are listed in Tab. \\ref{tab:massbbbar}. $M^{\\textmd{RL}}_{b\\bar{b}}$ and $M^{\\textmd{expt.}}_{b\\bar{b}}$ are our RL approximation result and the experiment value \\cite{Tanabashi2018} respectively. $\\Delta M^{\\textmd{RL}}_{b\\bar{b}}$ is the deviation, defined by\n",
      "The results in Tab. \\ref{tab:massccbar} and Tab. \\ref{tab:massbbbar} have two-sided meanings. On one hand, the RL approximation is reasonble for the charmonium and bottomonium system. The mass deviations are less than $3.5\\%$ for the charmonium and less than $1.1\\%$ for the bottomonium. However, the absolute errors do not decrease for the P-wave states, indicating that in the RL approximation some interactions are still lacking even for the heavy system. On the other hand, the results are stable when the parameters change. The uncertainties due to the varying of the parameters are small, less than $0.8\\%$ for the charmonium and less than $0.2\\%$ for the bottomonium. So the errors (defined by Eq.(\\ref{eq:deltaMccbar}) and Eq.(\\ref{eq:deltaMbbbar})) of the RL approximation could be estimated quantitatively.\n",
      "i.e., averaging the $C$-parity. In Tab. \\ref{tab:massccbar} and  Tab. \\ref{tab:massbbbar}, the difference of $\\Delta M^{\\textmd{RL}}_{c\\bar{c}}$ with $J^{PC}=1^{++}$ and $J^{PC}=1^{+-}$ are less than $31\\textmd{ MeV}$, and there is no difference for $\\Delta M^{\\textmd{RL}}_{b\\bar{b}}$ for these two states. So taking the $C-$parity average leads no more than $8\\textmd{ MeV}$ error for $\\Delta M^{\\textmd{RL}}_{c\\bar{b},1^+}$. \n",
      "tab:massExcited\n",
      "The relation Eq.(\\ref{eq:errorMbc}) is also supported by the masses of the radial excited states \\cite{Chang2019}. The center values of the excited state masses are cited in Tab. \\ref{tab:massExcited}. $(\\Delta M^{\\textmd{RL}}_{\\eta_{c}(2S)} + \\Delta M^{\\textmd{RL}}_{\\eta_{b}(2S)})/2 = -58\\textmd{ MeV}$, while $\\Delta M^{\\textmd{RL}}_{B^+_{c}(2S)} = -59\\textmd{ MeV}$. The direct calculation of the $B_c$ meson masses in the RL approximation and the mass errors due to the RL approximation are listed in Tab. \\ref{tab:masscbRL}. The modified mass is defined by\n",
      "tab:masscbRL\n",
      "The relation Eq.(\\ref{eq:errorMbc}) is also supported by the masses of the radial excited states \\cite{Chang2019}. The center values of the excited state masses are cited in Tab. \\ref{tab:massExcited}. $(\\Delta M^{\\textmd{RL}}_{\\eta_{c}(2S)} + \\Delta M^{\\textmd{RL}}_{\\eta_{b}(2S)})/2 = -58\\textmd{ MeV}$, while $\\Delta M^{\\textmd{RL}}_{B^+_{c}(2S)} = -59\\textmd{ MeV}$. The direct calculation of the $B_c$ meson masses in the RL approximation and the mass errors due to the RL approximation are listed in Tab. \\ref{tab:masscbRL}. The modified mass is defined by\n",
      "tab:masscball\n",
      "Our predictions of the $B_c$ mesons and the estimated errors are listed in the second column of Tab.\\ref{tab:masscball}. Hitherto the only experiment data for the $B_c$ spectrum is the pseudoscalar meson mass, as the production rate of the $B_c$ mesons is much lower than those of the charmonium and the bottomonium. Our results are consistent with the recent lQCD predictions (with $J^P = 0^-,\\,1^-,\\,0^+,\\,1^+$), which are listed in the forth column. Our results are also consistent with the quark model predictions. The quark model predictions from Ref. \\cite{Li2019} are listed in the third column, where the masses with $J^P =0^-,\\,1^-$ are the input values and others are the outputs. The mass splitting of the $1S$ state of our result is $M_{B_c(1^-)} - M_{B_c(0^-)} = 53\\textmd{ MeV}$, consistent with the lQCD result $55\\textmd{ MeV}$. The mass splittings of the $1P$ states of our results are $(M_{B_c(1^+_1)} + M_{B_c(1^+_2)})/2 - M_{B_c(0^+)} = 60\\textmd{ MeV}$ and $M_{B_c(2^+)} - M_{B_c(0^+)} = 90\\textmd{ MeV}$, while the quark model results are $53\\textmd{ MeV}$ and $73\\textmd{ MeV}$ respectively. Our results of the $B_c$ meson masses also have two-sided meanings. On one hand, we predict the masses of the $B_c$ mesons (with $J^P = 0^-,\\,1^-,\\,0^+,\\,1^+,\\,2^+$), providing a significant guide to the experimental search for the $B_c$ mesons. On the other hand, the consistency of our results with other predictions supports that the flavor dependent interaction pattern, Eq.(\\ref{eq:gluonfmodel}) $\\sim$ Eq.(\\ref{eq:gluonUltraviolet}), is reasonable. This pattern leads an error about $15\\textmd{ MeV}$ for the $B_c$ mesons.\n",
      "6 tables found.\n",
      "0 figures found.\n",
      "\n",
      "\n",
      "2001.00164v2\n",
      "tab:freq\n",
      "%Table~\\ref{tab:freq} is included in the input file; compare the\n",
      "tab:commands\n",
      "%Table~\\ref{tab:commands} is included in the input file; again, it is\n",
      "2 tables found.\n",
      "1 figures found.\n",
      "\n",
      "\n",
      "2001.00166v1\n",
      "0 tables found.\n",
      "1 figures found.\n",
      "\n",
      "\n",
      "2001.00167v1\n",
      "2 tables found.\n",
      "0 figures found.\n",
      "\n",
      "\n",
      "2001.00168v1\n",
      "table:galaxies\n",
      "The aim of this study is to look for a pixel-by-pixel correlation between gas and dust emissions and hence, our sample must have data available in both the radio and infrared (IR). In the near and mid-IR, beam sizes are small and hence the spatial resolution of instruments is not much of a problem. However, this is usually not the case for single-dish radio observations which is why we select a sample of galaxies such that it is sufficiently nearby, in order to avoid any contamination in the observed spectra due to rotation effects. We have selected NGC 3184 and NGC 7793 which are classified as a `grand design spiral' and a `flocculent spiral', respectively, according to the scheme of morphological galaxy classification given by \\cite{Elmegreen+1982}. The difference in properties such as star-formation rate, inclination angle, etc. make these two ideal for a study of the trend in gas-dust correlations at high linear resolution among galaxies having very different spiral structures, but closely related in morphological type (Table \\ref{table:galaxies}). The findings reported here are part of an ongoing larger project making use of archival data for nearby galaxies and we present these two example galaxies as a pilot study. \\\\\n",
      "A compilation of some known properties of the two galaxies is presented in Table \\ref{table:galaxies}. Historically, NGC 3184 has been the more widely studied galaxy of the two. However, a study of gas-dust correlations by separating the warm and cold components of HI gas has not been attempted before in either case. Hence, while both galaxies are individually interesting due to the similarities they possess with the Milky Way and M33, respectively, we wanted to study how the gas-dust interactions vary within each galaxy with a change in physical properties. We have, in fact, separated the spiral arm locations to further investigate the possible contribution of local effects in NGC 3184.\\\\\n",
      "table:telescopes\n",
      "A comparative chart of the resolutions at different wavelengths for the four surveys (THINGS, SINGS, KINGFISH, HERACLES) is presented in Table \\ref{table:telescopes}. An overlay of the THINGS radio HI intensity and velocity dispersion contours on an 8 $\\mu$m SINGS image is shown in Figure \\ref{overlays_3184} for NGC 3184 and Figure \\ref{contours_7793} for NGC 7793.\\\\\n",
      "\\textbf{Image convolution and re-sizing:} As seen in Table \\ref{table:telescopes}, the radio, IR and CO data available at multiple wavelength bands have different sizes and resolutions. Therefore, in order to bring all the images to equal footing for comparison, we select the largest beam size available for each galaxy in our sample and then convolve the rest of the data to this beam size using standard tasks in the Astronomical Image Processing System (AIPS) software. First, we manually apply masks to all available images such that we can avoid contamination from the Galactic foreground stars, e.g. as seen in Figures \\ref{3HImaps} \\& \\ref{7HImaps}. The images are then convolved such that the output attains the resolution of the largest beam among all the available gas/dust maps. Once all the images for a particular galaxy have been convolved to the new beam size, we re-sample all the image data such that the pixel size becomes equal to the beam size, i.e. one pixel per beam. For example, a radio image with an original beam size of 7.51$^{\\prime\\prime}\\times$6.93$^{\\prime\\prime}$ for NGC 3184 has 1024$\\times$1024 pixels, with a pixel size of 1.5$^{\\prime\\prime}$ and it has been convolved such that we get a final output beam size of 13.4$^{\\prime\\prime}\\times$13.4$^{\\prime\\prime}$, which is in accordance with the highest beam size in the sample belonging to the CO map. We divide the beam size by the pixel size (13.4/1.5) to get the number of pixels in a beam (=8.9) and then divide the total number of pixels in the image by this number (1024/8.9) to get the new number of image pixels (=115$\\times$115), for re-sampling to make the pixel size and beam size equal. Moreover, the size of the images and their reference pixels need to be re-aligned since our objective is to do a pixel-by-pixel comparative study and this has been done using the Common Astronomy Software Applications (CASA) package. Thus, we bring all the images (including the cold/warm HI maps) to a uniform size and geometry for meaningful comparison.\\\\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr_irHI\n",
      "Figures \\ref{3plots_irHI8} \\& \\ref{7plots_irHI8} show the gas-dust correlations for the 8 $\\mu$m dust band in NGC 3184 and NGC 7793, respectively. The four panels in Figure \\ref{3plots_irHI8} correspond to the separated warm/cold HI gas, total HI gas, molecular CO gas and total (atomic+molecular) gas column densities with respect to 8 $\\mu$m dust emission in NGC 3184. The spiral arm locations have been marked separately in the total gas-dust correlation plot for NGC 3184 which have been identified using a combination of 3.6 $\\mu$m and H$\\alpha$ images available in the SINGS data archive \\citep{Kennicutt+2003}. Since the CO data was unavailable for NGC 7793 in the public archives, we have only shown the separated warm/cold HI and the total HI gas column density vs 8 $\\mu$m dust emission for this galaxy in Figure \\ref{7plots_irHI8}. Being a flocculent spiral, the arms in NGC 7793 are distributed throughout the galaxy and hence we haven't been able to separate them from the inter-arm locations. While we have not shown all the gas-dust correlation plots for individual IR bands here, we have used the non-parametric Spearman's ($\\rho$) and Kendall's ($\\tau$) rank correlation coefficients to quantify the relationship between various observed parameters and the coefficient values are presented in Tables \\ref{corr_irHI} \\& \\ref{corr_mfir}. In addition, we have compared the total N(HI) gas-dust correlations for the two galaxies at different IR bands as shown in Figure \\ref{plots_overlay}. \\\\\n",
      "We see that the cold HI is relatively weakly correlated with the dust emission as compared to the warm HI gas (Table \\ref{corr_irHI}). This means that the total N(HI) correlation which is seen with individual dust bands is mainly due to the warm gas component in both galaxies (Figures \\ref{3plots_irHI8} \\& \\ref{7plots_irHI8}). The molecular gas in NGC 3184 shows a good positive correlation ($\\sim$0.7) with all IR bands and it also dominates the total gas (atomic+molecular) vs dust correlation which is tighter ($\\sim$0.8) than either individual case as seen in Figure \\ref{3plots_irHI8}. If we only look at the spiral arm locations in NGC 3184, we do not see any correlation due to the narrow range of IR emission values covered. We see the overall positive correlation only when we consider all locations. Moreover, the spiral arms do not have any preferential cold/warm gas dominance in NGC 3184. These locations are, of course, concentrated towards higher dust emission regions due to the higher dust concentration in the spiral arms (Figure \\ref{3plots_irHI8}).\\\\\n",
      "corr_mfir\n",
      "Figures \\ref{3plots_irHI8} \\& \\ref{7plots_irHI8} show the gas-dust correlations for the 8 $\\mu$m dust band in NGC 3184 and NGC 7793, respectively. The four panels in Figure \\ref{3plots_irHI8} correspond to the separated warm/cold HI gas, total HI gas, molecular CO gas and total (atomic+molecular) gas column densities with respect to 8 $\\mu$m dust emission in NGC 3184. The spiral arm locations have been marked separately in the total gas-dust correlation plot for NGC 3184 which have been identified using a combination of 3.6 $\\mu$m and H$\\alpha$ images available in the SINGS data archive \\citep{Kennicutt+2003}. Since the CO data was unavailable for NGC 7793 in the public archives, we have only shown the separated warm/cold HI and the total HI gas column density vs 8 $\\mu$m dust emission for this galaxy in Figure \\ref{7plots_irHI8}. Being a flocculent spiral, the arms in NGC 7793 are distributed throughout the galaxy and hence we haven't been able to separate them from the inter-arm locations. While we have not shown all the gas-dust correlation plots for individual IR bands here, we have used the non-parametric Spearman's ($\\rho$) and Kendall's ($\\tau$) rank correlation coefficients to quantify the relationship between various observed parameters and the coefficient values are presented in Tables \\ref{corr_irHI} \\& \\ref{corr_mfir}. In addition, we have compared the total N(HI) gas-dust correlations for the two galaxies at different IR bands as shown in Figure \\ref{plots_overlay}. \\\\\n",
      "When we look at the MIR vs FIR correlations in both galaxies, we find an overall similar positive correlation trend between the different dust bands indicating a mixture of hot and cold dust components. While the correlation coefficients among the different 8, 24, 100 \\& 160 $\\mu$m emissions have similar values for both galaxies, there is a slight difference in case of the correlation coefficients involving 70 $\\mu$m emission. For instance, the 70 vs 100 $\\mu$m Spearman's rank correlation coefficient for NGC 3184 is $\\sim$0.6 while it is $\\sim$0.8 for NGC 7793 (Table \\ref{corr_mfir}). So, while a large fraction of the dust particles responsible for the 70 $\\mu$m emission may come from similar grains which show emission at 24 $\\mu$m with sensitivity to star-formation effects \\citep{Dale+2005,Calzetti+2005}, the 24 $\\mu$m may also arise from diffuse cirrus especially in inter-arm regions \\citep{Foyle+2010}. Therefore, the relatively weaker correlation between other dust bands and 70 $\\mu$m  emission in NGC 3184 as compared to NGC 7793 might be due to higher contribution towards the cold dust emission from the inter-arm regions at the other IR bands as opposed to the warm dust emission at 70 $\\mu$m. These observations imply that NGC 7793 has a higher cold dust fraction as compared to NGC 3184.\\\\\n",
      "4 tables found.\n",
      "8 figures found.\n",
      "\n",
      "\n",
      "2001.00169v1\n",
      "UnicodeDecodeError occurred. File could not be loaded.\n",
      "Document header could not be identified.\n",
      "\n",
      "\n",
      "2001.00172v1\n",
      "Document header could not be identified.\n",
      "\n",
      "\n",
      "2001.00173v1\n",
      "tbl:example\n",
      "File not found: withoutbeads.pdf - [Errno 2] No such file or directory: 'withoutbeads.pdf'\n",
      "File not found: noise.pdf - [Errno 2] No such file or directory: 'noise.pdf'\n",
      "1 tables found.\n",
      "20 figures found.\n",
      "\n",
      "\n",
      "2001.00174v1\n",
      "UnicodeDecodeError occurred. File could not be loaded.\n",
      "Document header could not be identified.\n",
      "\n",
      "\n",
      "2001.00175v2\n",
      "table:mu\n",
      "In Table \\ref{table:mu}, we list sixteen charmonia so far established, where $Y(4220)$ and $\\psi(4415)$ are treated as the scaling points of our unquenched potential model and are assigned to charmonia $\\psi(4S)$ and $\\psi(5S)$, respectively. In order to obtain the best agreement with the experimental mass of established charmonia, we mainly adjust three parameters $\\mu$, $b$, and $c$ of the screened confining potential in Eq.~(\\ref{eq:potential}). Here, we define a $\\chi^2$ value to describe our theoretical deviation from experimental data, which is\n",
      "where $M_{Exp}$, $M_{Th}$ and $M_{Er}$ are experimental value, theoretical value and error, respectively. An overall $M_{Er}$ = 5.0 MeV is chosen for convenience. From Table \\ref{table:mu}, we can see that the established charmonium spectrum can be described well when the parameter $\\mu$ is located in the range of $0.11\\sim0.15$, where the masses of charmonia below 4.5 GeV have no obvious difference with different screened parameters. Whereas, when we further focus on higher charmonia such as $\\psi(6S)$ and $\\psi(7S)$, their theoretical mass will change about 100 MeV with the increase of a $\\mu$ value from 0.11 to 0.15. This phenomenon can be understandable by the nature of the screened confining interaction.\n",
      "In Fig.~\\ref{fig:potential}, we plot the screened confining plus one-gluon-exchange potential with different screened parameters, in which the root-mean-squared(RMS) radius of $\\psi(4S)$ and $\\psi(6S)$ are labeled as an illustration. One can see that the potential behavior is almost the same when the distance $r$ is smaller than the RMS radius of $\\psi(4S)$, $r_{4S}$. However, the potentials with different $\\mu$ values begin to obviously separate at the position of $r_{6S}$, at which the screened effect plays a very important role. As a result, the predictions of higher charmonia around 4.6 GeV based on the present experimental data in Table \\ref{table:mu} have large uncertainties, and this also reflects on how poor we know about strong interactions. In short, a reasonable description of charmonium spectrum around 4.6 GeV is not an easy work, and we need more valuable experimental hints.\n",
      "In the present experimental status, the charmoniumlike state $Y(4630)$ or $Y(4660)$ can be considered as a higher scaling point of a charmonium spectrum. Therefore, in this section, we will explore whether these $Y$ states can be treated as higher charmonia. We notice that both $Y(4220)$ and $Y(4660)$ have been discovered in the hidden charm process $e^+e^-\\to\\psi(2S)\\pi^+\\pi^-$ \\cite{Wang:2007ea,Ablikim:2017oaf}. And if the nature of $Y(4220)$ is a charmonium $\\psi(4S)$, then the $Y(4660)$ should also be a good candidate of a higher charmonium. Actually, there have been some theoretical works discussing the possibilities of $Y(4660)$ as a charmonium state after its experimental discovery \\cite{Ding:2007rg,Li:2009zu}. In Ref. \\cite{Li:2009zu}, Li and Chao suggested that $Y(4660)$ is the candidate of $6^3S_1$ $c\\bar{c}$ state by analyzing their calculated mass, whose value is 4608 MeV and smaller than the experimental mass of $4652\\pm10\\pm8$ MeV \\cite{Wang:2014hta}. From our results listed in Table \\ref{table:mu}, the mass of $\\psi(6S)$ is predicted to be 4542 to 4640 MeV with different $\\mu$ values, which are also lower than the measured mass of $Y(4660)$. We also notice that the experimental mass difference $M_{Y(4660)}-M_{\\psi(4415)}=231$ MeV is greater than $M_{\\psi(4415)}-M_{Y(4220)}=191$ MeV. If the property of $Y(4660)$ is dominated by a $\\psi(6S)$ state, according to experiences in the quark model, the above mass diferences are very abnormal because the mass gap should be expected to decrease with the increase of radial quantum numbers. Hence the real position of $\\psi(6S)$ in charmonium spectrum is still an open problem.\n",
      "where the input of $m_{6S}$ and $m_{5D}$ are dependent on our prediction from the unquenched potential model. We note that when taking screened parameter $\\mu=0.12$, the masses of pure $6S$ and $5D$ $c\\bar{c}$ states are 4615 and 4648 MeV as shown in Table \\ref{table:mu}, respectively. This result roughly coincides with the mass of two fitted resonances, so we choose the screened parameter $\\mu=0.12$ to describe higher charmonia around 4.6 GeV. The complete parameters adopted in the unquenched potential model are listed as follows,\n",
      "table:fitparameter\n",
      "With the above preparation, a combined fit to the experimental data both of the $e^+e^-\\rightarrow\\psi(2S)\\pi^+\\pi^-$ and $e^+e^-\\rightarrow\\Lambda_c\\bar{\\Lambda}_c$ \\cite{Wang:2014hta,Ablikim:2017oaf,Pakhlova:2008vn,Ablikim:2017lct} is achieved, and the corresponding fitted line shapes are shown in Fig.~\\ref{fig:2spipi} and Fig.~\\ref{fig:lambdac}. The relevant fitted parameters are listed in Table \\ref{table:fitparameter}, where the $\\chi^2/d.o.f$=1.49 is obtained. To our surprise, the ``platform\" behavior of experimental data of $e^+e^-\\rightarrow\\Lambda_c\\bar{\\Lambda}_c$ near threshold can be indeed reproduced in our proposed two-resonance scheme, which can be seen in Fig. \\ref{fig:lambdac}. Meanwhile, the experimental data of $e^+e^-\\rightarrow\\psi(2S)\\pi^+\\pi^-$ are also described well as shown in Fig. \\ref{fig:2spipi}, where a precise data point at 4.6 GeV from BESIII also provides the possible evidence to support our conjecture of the existence of a new charmoniumlike structure around 4.6 GeV. Anyway, more precise experimental measurements are very important for clarifying the structures %existed\n",
      "{Finally, we briefly discuss two observed decay channels $\\psi'_{6S-5D}$/$\\psi''_{6S-5D} \\to \\psi(2S)\\pi^+\\pi^-$ and $\\psi'_{6S-5D}$/$\\psi''_{6S-5D} \\to \\Lambda_c\\bar{\\Lambda}_c$. In our combined fit to experimental data of the $e^+e^-\\rightarrow\\psi(2S)\\pi^+\\pi^-$ and $e^+e^-\\rightarrow\\Lambda_c\\bar{\\Lambda}_c$, we obtain four parameters $\\mathcal{R}^{Y_1}_{\\psi(2S)\\pi^+\\pi^-}$, $\\mathcal{R}^{Y_2}_{\\psi(2S)\\pi^+\\pi^-}$, $\\mathcal{R}^{Y_1}_{\\Lambda_c\\bar{\\Lambda}_c}$ and $\\mathcal{R}^{Y_2}_{\\Lambda_c\\bar{\\Lambda}_c}$, which correspond to the product of di-lepton width and branching ratio of decay mode $\\psi(2S)\\pi^+\\pi^-$ or $\\Lambda_c\\bar{\\Lambda}_c$ of charmonium  $\\psi_{6S-5D}$ state. Generally, the width of the di-lepton of charmonia has order of keV \\cite{Tanabashi:2018oca}. Thus, we can roughly estimate the branching ratio of $\\psi'_{6S-5D}$/$\\psi''_{6S-5D} \\to \\psi(2S)\\pi^+\\pi^-$ and $\\psi'_{6S-5D}$/$\\psi''_{6S-5D} \\to \\Lambda_c\\bar{\\Lambda}_c$ to be around $10^{-4}\\sim10^{-3}$ and $10^{-3}\\sim10^{-2}$, respectively, according to the fitted value of $\\mathcal{R}^{Y}$ listed in Table \\ref{table:fitparameter}. These results are consistent with the charmonium decay behavior since the $\\Lambda_c\\bar{\\Lambda}_c$ is an open charm decay mode while the $\\psi(2S)\\pi^+\\pi^-$ is a hidden charm final states. In future work, we will try to explore the dynamics mechanisms involved in these decay channels, which is another approach to decode the properties of charmonia around 4.6 GeV. }\n",
      "table:mixing\n",
      "In Fig.~\\ref{fig:mixing}, we present the dependence of masses of $\\psi'_{6S-5D}$ and $\\psi''_{6S-5D}$ on mixing angle $\\theta$. As the $\\theta$ increases gradually, it can be seen that the mass of $\\psi'_{6S-5D}$ is decreased and the mass of $\\psi''_{6S-5D}$ is oppositely enhanced. When mixing angle $\\theta=\\pm34^{\\circ}$, we were surprised to find that the theoretical masses of $\\psi'_{6S-5D}$ and $\\psi''_{6S-5D}$ are perfectly consistent with our fitting resonance masses, whose comparisons are shown in Table~\\ref{table:mixing}. It is no doubt that this mass consistency provides us great confidence to believe these charmoniumlike $Y$ states around 4.6 GeV can be treated as higher members in $J/\\psi$ family.\n",
      "tab:table1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We firstly calculate the total and partial widths of the open charm two-body strong decays of pure $\\psi(6S)$ and $\\psi(5D)$ states, which are summarized in Table \\ref{tab:table1}. The total width of $\\psi(6S)$ is estimated to be 28.5 MeV, which is in full accordance with the fitted width of $29.8\\pm8$ MeV of the charmoniumlike $Y_1$ state. After considering the $S$-$D$ mixing, we find the total widths of $\\psi'_{6S-5D}$ and $\\psi''_{6S-5D}$ are not sensitive to the mixing angle as shown in Fig. \\ref{fig:mixing}. Thus, both mass and width of the $\\psi'_{6S-5D}$ dominated by the $c\\bar{c}$ component of $6^3S_1$ are consistent with the $Y_1$ state. This gives us\n",
      "When the mixing angle of the $6S$-$5D$ mixture is $\\pm34^{\\circ}$, the mass and total width of $\\psi''_{6S-5D}$ are 4675 and 30 MeV, respectively, where the theoretical mass is close to 4676$\\pm$7 MeV of the fitted $Y_2$ state, but the width is smaller than the fitted result of $85.7\\pm15$ MeV. However, we should emphasize here that the width of $Y_2$ resonance in our present fit may be not accurate due to the possible influences from higher charmonia above the peak of $\\psi''_{6S-5D}$. For example, we notice that the mass of pure $\\psi(7S)$ state is 4.726 GeV as listed in Table \\ref{tab:table1}, which can easily interfere with the resonance $Y_2$ to change its Breit-Wigner distribution. The present experimental data cannot allow us to include their contributions in the fit of the $e^+e^-\\rightarrow\\psi(2S)\\pi^+\\pi^-$ and $e^+e^-\\rightarrow\\Lambda_c\\bar{\\Lambda}_c$. Hence, the richer experimental measurements will be helpful to clarify this point in the future.\n",
      "Here, we focus on six higher vector charmonia, which are $\\psi(7S)$, $\\psi(8S)$, $\\psi(9S)$, $\\psi(6D)$, $\\psi(7D)$, and $\\psi(8D)$. The numerical results of their masses and decay behaviors are listed in Table~\\ref{tab:table1}. We can see that their masses are all located in the energy region of 4.70 to 4.90 GeV, which indicates that the charmonium spectroscopy above 4.6 GeV has become very dense.\n",
      "Another interesting feature is that these high excited vector charmonia above 4.6 GeV are relatively narrow states although more open charm channels have opened. Their total widths are about 10 to 30 MeV as shown in Table ~\\ref{tab:table1}. The main reason for this phenomenon is the node effect. In the QPC model, the decay amplitude is proportional to the overlap integration of the spatial wave function of initial and final states. As we all know, the meson radial wave function with the radial quantum number $n$ has $n$-1 nodes, which means the overlap integration of the highly radial excited charmonia may cancel each other in the positive and negative value area more strongly due to the existence of more nodes. Therefore, the interesting properties in the charmonium energy region above 4.6 GeV can be left to experimentalists to test in the future.\n",
      "According to our predictions of open charm decay behaviors listed in Table~\\ref{tab:table1}, we can provide some dominant decay channels to search for the higher charmonia above 4.6 GeV in the future experiments. It can be seen that there are a plenty of open charm modes allowed by the phase space, where apart from two $S$-wave charmed mesons, the channels involving $S$-wave and $P$-wave charmed mesons are also important. For the higher $S$-wave charmonia above 4.6 GeV, we find the dominant decay channels are $DD_1(2430)$, $D^*D_0^*(2400)$, $D^*D_1(2430)$, $D^*D_2^*(2460)$, $DD^*(2600)$, and $D^*D_1(2420)$. However, the channels, $D^*D^*$, $DD$, $D^*D_1(2420)$, $D^*D_1(2430)$, $D^*D_2^*(2460)$, and $DD_1(2430)$ should be considered\n",
      "File not found: diagram.pdf - [Errno 2] No such file or directory: 'diagram.pdf'\n",
      "File not found: figpotential.pdf - [Errno 2] No such file or directory: 'figpotential.pdf'\n",
      "File not found: fig2spipi.pdf - [Errno 2] No such file or directory: 'fig2spipi.pdf'\n",
      "File not found: figlambdac.pdf - [Errno 2] No such file or directory: 'figlambdac.pdf'\n",
      "File not found: figspectrum.pdf - [Errno 2] No such file or directory: 'figspectrum.pdf'\n",
      "File not found: figmixing.pdf - [Errno 2] No such file or directory: 'figmixing.pdf'\n",
      "File not found: figdsds1the.pdf - [Errno 2] No such file or directory: 'figdsds1the.pdf'\n",
      "File not found: figdsds1exp.pdf - [Errno 2] No such file or directory: 'figdsds1exp.pdf'\n",
      "4 tables found.\n",
      "8 figures found.\n",
      "\n",
      "\n",
      "2001.00176v2\n",
      "File not found: SK-inverse2 - [Errno 2] No such file or directory: 'SK-inverse2'\n",
      "File not found: S1SK - [Errno 2] No such file or directory: 'S1SK'\n",
      "File not found: SKK-pic - [Errno 2] No such file or directory: 'SKK-pic'\n",
      "0 tables found.\n",
      "8 figures found.\n",
      "\n",
      "\n",
      "2001.00177v2\n",
      "tab1\n",
      "We use a simplified model to study wave reflection and transmission at interface of convective region and stably stratified region (e.g. radiative zone in star or stratification layer in gaseous planet). Inertial wave in convective region and gravito-inertial wave in stably stratified region are considered. We begin with polar area and then extend to any latitude. Six cases are discussed (see Table \\ref{tab1}), and in Case 2 both waves co-exist in both regions. Four configurations are further discussed for Case 2. The angles and energy ratios of wave reflection and transmission are calculated. It is found that wave propagation and transmission depend on the ratio of buoyancy frequency to rotational frequency. In a rapidly rotating star or planet wave propagates across interface and most of energy of incident wave is transmitted to the other region, but in a slowly rotating star or planet wave transmission is inhibited.\n",
      "All the six cases are summarized in Table \\ref{tab1}. In Cases 1 and 4 wave does not exist. In Case 2 waves exist in both regions. In Case 3, 5 or 6 only one wave exists in one region and in the other region wave amplitude exponentially decays away from interface, i.e. the so-called evanescent region. For example, Cases 5 and 6 take place in the solar interior.\n",
      "We studied the six cases as listed in Tabel \\ref{tab1}, and in Case 2 ($N^2<\\omega^2<4\\Omega^2$) both convective and stratified regions support wave motion. We then focused on the four configurations in Case 2 for solar-type and massive stars. It is found that \n",
      "1 tables found.\n",
      "7 figures found.\n",
      "\n",
      "\n",
      "2001.00178v2\n",
      "tab:Stokesd\n",
      "%The resulting  {$(v_1,v_2)$} and $p$ are presented in Table \\ref{tab:Stokesd} for time step $\\tau=10^{-2}$.\n",
      "tab:errors\n",
      "If we focus on the time step size $\\tau=\\frac{1}{512}$ and perform 1024 time steps, we obtain the final $L^2$-errors for the pressure listed in Table \\ref{tab:errors}. \\revision{We have denoted by bold the spaces resulting in the lowest numerical error of the pressure.}\n",
      "From Table \\ref{tab:errors} we can read that the higher continuity spaces result in a lower computational cost. }\n",
      "File not found: p190 - [Errno 2] No such file or directory: 'p190'\n",
      "2 tables found.\n",
      "11 figures found.\n",
      "\n",
      "\n",
      "2001.00179v3\n",
      "table:databases_entireFace\n",
      "This manipulation creates entire non-existent face images. Table~\\ref{table:databases_entireFace} summarises the main publicly available databases for research on detection of image manipulation techniques relying on entire face synthesis. Four different databases are of relevance here, all of them based on the same GAN architectures: ProGAN~\\cite{pgan} and StyleGAN~\\cite{Karras_2019_CVPR}. It is interesting to remark that each fake image may be characterised by a specific GAN fingerprint just like natural images are identified by a device-based fingerprint (i.e., PRNU). In fact, these fingerprints seem to be dependent not only of the GAN architecture, but also of the different instances of it~\\cite{marra2019gans,albright_CVPR2019,richa_vatsa_2020}.\n",
      "In addition, as indicated in Table~\\ref{table:databases_entireFace}, it is important to note that the four mentioned databases only contain fake images generated using the GAN architectures discussed. In order to perform fake detection experiments on this manipulation group, researchers need to obtain real face images from other public databases such as CelebA~\\cite{celeba}, FFHQ~\\cite{Karras_2019_CVPR}, CASIA-WebFace~\\cite{yi2014learning}, and VGGFace2~\\cite{cao2018vggface2}, among others.\n",
      "table:relatedWorks_entireFaceSynthesis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Different studies have recently evaluated the difficulty of detecting whether faces are real of artificially generated. Table~\\ref{table:relatedWorks_entireFaceSynthesis} shows a comparison of the most relevant approaches in this area. For each study, we include information related to the method, classifiers, best performance, and databases considered. We highlight in \\textbf{bold} the best results achieved for each public database. It is important to remark that in some cases, different evaluation metrics are considered, e.g., Area Under the Curve (AUC) or Equal Error Rate (EER), which complicates the comparison among the studies.\n",
      "Fake detection systems inspired in steganalysis have also been studied. Nataraj \\textit{et al.} proposed in~\\cite{nataraj2019detecting} a detection system based on a combination of pixel co-occurrence matrices and Convolutional Neural Networks (CNN). Their proposed approach was initially tested through a database of various objects and scenes created through CycleGAN~\\cite{zhu2017unpaired}. Besides, the authors performed an interesting analysis to see the robustness of the proposed approach against fake images created through different GAN architectures (CycleGAN vs. StarGAN), with good generalisation results. This detection approach was implemented later on in~\\cite{2019_Arxiv_GANRemoval_Tolosana} considering images from the 100K-Faces database, achieving an EER of 12.3\\% for the best fake detection performance. This result is remarked in \\textit{italics} in Table~\\ref{table:relatedWorks_entireFaceSynthesis} to indicate that it was not provided in the original paper. \n",
      "table:databases_faceSwap\n",
      "Since publicly available fake databases such as the UADFV database~\\cite{li2018ictu}, up to the recent Celeb-DF and DFDC databases~\\cite{li2019celebdf,dolhansky2019deepfake}, many visual improvements have been carried out, increasing the realism of fake videos. As a result, identity swap databases can be divided into two different generations. Table~\\ref{table:databases_faceSwap} summarises the main details of each public database, grouped in each generation. As can be seen, in this type of facial manipulation both real and fake videos are usually included in the databases.\n",
      "table:relatedWorks_faceSwapping\n",
      "The development of novel methods to detect identity swap manipulations is continuously evolving. Table~\\ref{table:relatedWorks_faceSwapping} provides a comparison of the most relevant detection approaches in this area. For each study we include information related to the method, classifiers, best performance, and databases for research. We highlight \\textbf{in bold} the best results achieved for each public database. It is important to remark that in some cases, different evaluation metrics are considered (e.g., AUC and EER), which complicates the comparison among studies. Finally, the results highlighted in \\textit{italics} indicate the generalisation capacity of the detection systems against different unseen databases, i.e., those databases were not considered for training. These results have been extracted from~\\cite{li2019celebdf} and were not included in the original publications.\n",
      "Fake detection systems based on facial expressions and head movements have also been proposed in the literature. Yang \\textit{et al.} observes in~\\cite{yang2019exposing} that some DeepFakes are created by splicing synthesised face regions into the original image, and in doing so, introducing errors that can be revealed when 3D head poses are estimated from the face images. Thus, they performed an study based on the differences between head poses estimated using a full set of facial landmarks (68 extracted from DLib~\\cite{king2009dlib}) and those in the central face regions to differentiate DeepFakes from real videos. Once these features are extracted and normalised (mean and standard deviation), a SVM is considered for the final classification. Their proposed approach was originally evaluated with the UADFV database, achieving a final 89.0\\% AUC. However, this pre-trained model (using UADFV database) seems not to generalise very well to other databases as depicted in Table~\\ref{table:relatedWorks_faceSwapping}. \n",
      "In conclusion, although many different approaches have been proposed in the literature, they all show poor generalisation results to unseen databases, as indicated in Table~\\ref{table:relatedWorks_faceSwapping}. In addition, we also highlight the poor detection results achieved by most approaches on the DeepFake databases of the \\nth{2} generation with results below 60\\% AUC.\n",
      "table:relatedWorks_facialAttributes\n",
      "Attribute manipulations have been originally studied in the field of face recognition in order to see how robust biometric systems are against physical factors such as plastic surgery, cosmetics, makeup or occlusions~\\cite{kim2005effective,dantcheva2012can,kose2015facial,majumdar2019evading,christianIEEEAccess2019,9109348}. However, it has been the recent success of mobile applications such as FaceApp that has motivated the research community to detect digital face attribute manipulations. Table~\\ref{table:relatedWorks_facialAttributes} provides a comparison of the most relevant approaches in this area. We include for each study information related to the method, classifiers, best performance, and databases for research.\n",
      "To summarise this section, we can see that the core of most attribute manipulation detection systems are based on deep learning technology, providing in general very good results close to 100\\% accuracy, as indicated in Table~\\ref{table:relatedWorks_facialAttributes}. This is mainly produced due to the GAN-fingerprint information present in fake images. However, as indicated in the entire face synthesis manipulation, recent studies have been proposed in the literature to remove such GAN fingerprints from the fake images while keeping very realistic appearance~\\cite{2019_Arxiv_GANRemoval_Tolosana,cozzolino2019spoc}, which represent a challenge even for the most advanced manipulation detectors.\n",
      "table:relatedWorks_facialExpression\n",
      "Table~\\ref{table:relatedWorks_facialExpression} provides a comparison of the most relevant approaches in the area of expression swap detection. For each study we include information related to the method, classifiers, best performance, and databases. We highlight in \\textbf{bold} the best results achieved for the only public database, FaceForensics++. It is important to remark that in some cases, different evaluation metrics are considered (e.g., AUC and EER), which makes it difficult to perform a fair comparison among the studies.\n",
      "6 tables found.\n",
      "3 figures found.\n",
      "\n",
      "\n",
      "2001.00181v2\n",
      "Document header could not be identified.\n",
      "\n",
      "\n",
      "2001.00182v1\n",
      "tab:notations\n",
      "All notations we adopt are summarized in Table\\,\\ref{tab:notations}; we also mention that the term  ``packet transmission'' is often used interchangeably with ``bearer request''. \n",
      "tab:bearer_establishment\n",
      "The first and second column of Table~\\ref{tab:bearer_establishment} show the average  (over 20 runs) and the standard deviation of the time elapsing between the first and the last packet processed by the MME with one UE and two UEs, respectively. In the case of two UEs, we only consider the data relative to the bearer establishment of the first UE. In the third column, we demonstrate that it is fair to assume that a PS policy is in place. Indeed, considering the time in the second column, and halving the time in which the procedures of the two UEs overlap, we obtain a value that is very close to the one in the first column.\n",
      "tab:message_per_bearer\n",
      "tab-KS\n",
      "4 tables found.\n",
      "11 figures found.\n",
      "\n",
      "\n",
      "2001.00183v4\n",
      "0 tables found.\n",
      "2 figures found.\n",
      "\n",
      "\n",
      "2001.00184v1\n",
      "0 tables found.\n",
      "7 figures found.\n",
      "\n",
      "\n",
      "2001.00185v5\n",
      "improvementtable\n",
      "and equality occurs when $\\cos(\\theta)=t^{\\alpha+1,\\alpha+\\varepsilon}_{1,d}.$ In this section, we apply Proposition~\\ref{liftsphere} to Levenshtein's optimal polynomials for various angles $\\theta $ (columns of Table~\\ref{improvementtable}) and obtain\n",
      "with maximal $r$ (in Proposition~\\ref{liftsphere}), where $\\alpha_n(\\theta)$ are the entries of the table. Note that in Table~\\ref{improvementtable}, the improvement factors appear to gradually become independent of $\\theta$ as $n$ enlarges. We conjecture that they tend to $\\frac{1}{e}$ as $n\\rightarrow\\infty$.\n",
      "2 tables found.\n",
      "7 figures found.\n",
      "\n",
      "\n",
      "2001.00186v1\n",
      "Document header could not be identified.\n",
      "\n",
      "\n",
      "2001.00187v1\n",
      "table:performance\n",
      "table:ablation\n",
      "table:attention\n",
      "3 tables found.\n",
      "7 figures found.\n",
      "\n",
      "\n",
      "2001.00188v3\n",
      "tabPlato\n",
      "\tLet us now consider the maximal quantum correlations. We note that there are 15 possible pairs of Platonic solids (including when both solids are the same). For each of these 15 cases, we have constructed the Bell inequality \\eqref{gen}, computed the quantum value \\eqref{corr} and compared it to the maximal quantum value obtained via the first level of the hierarchy of quantum correlations. We find that the quantum strategy based on the Platonic solids always is optimal. In Table~\\ref{tabPlato} we compare the maximal quantum correlations with the local bound. We see that in all cases except for that of two tetrahedra, two octahedra and two cubes, the quantum correlations violate the local bound\\footnote{Since the relative angle between the two Platonic solids matters, we specify that the vertices of the Platonic solids where chosen to be the ones given by the software Mathematica's built-in function \"PolyhedronData\".}\\footnote{We remark that the visibility required for a violation in the presence of white noise is the ratio between the local and quantum bounds.}. Moreover, due to the structure of the Platonic Bell inequalities, the maximal quantum value of $\\mathcal{B}_{\\text{Plato}}$ is a simple rational number.\n",
      "1 tables found.\n",
      "6 figures found.\n",
      "\n",
      "\n",
      "2001.00190v2\n",
      "table1\n",
      "1 tables found.\n",
      "4 figures found.\n",
      "\n",
      "\n",
      "2001.00191v1\n",
      "UnicodeDecodeError occurred. File could not be loaded.\n",
      "Document header could not be identified.\n",
      "\n",
      "\n",
      "2001.00193v1\n",
      "0 tables found.\n",
      "9 figures found.\n",
      "\n",
      "\n",
      "2001.00194v3\n",
      "0 tables found.\n",
      "3 figures found.\n",
      "\n",
      "\n",
      "2001.00195v2\n",
      "tab:list-of-regex-strings\n",
      "File not found: #3 - [Errno 2] No such file or directory: '#3'\n",
      "File not found: #3 - [Errno 2] No such file or directory: '#3'\n",
      "1 tables found.\n",
      "6 figures found.\n",
      "\n",
      "\n",
      "2001.00196v3\n",
      "0 tables found.\n",
      "0 figures found.\n",
      "\n",
      "\n",
      "2001.00197v1\n",
      "table_param_num\n",
      "we perform 9 runs which  are summarized in Table~\\ref{table_param_num}.\n",
      "motivated runs listed in Table~\\ref{table_param_num}. In top panel the dashed lines represent the \n",
      "a function of time for the numerically motivated runs (Table~\\ref{table_param_num}). \n",
      "while bottom panel displays the disc radius for all runs listed in Table~\\ref{table_param_num}. \n",
      "table_param\n",
      "influence of initial conditions. They are summarized in Table~\\ref{table_param}.\n",
      "all the runs listed in Table~\\ref{table_param} except for run $R9$ which do not present outflow. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tables found.\n",
      "21 figures found.\n",
      "\n",
      "\n",
      "2001.00202v3\n",
      "0 tables found.\n",
      "0 figures found.\n",
      "\n",
      "\n",
      "2001.00203v1\n",
      "tab1\n",
      "These operators are evaluated using completely symmetric symmetric spin-isospin states $\\vert W_B\\rangle$~\\cite {Clo}. We obtained the quark model matrix elements up to second order corrections (two-body terms) listed in Table~\\ref{tab1}, \n",
      "To derive from the quark level matrix elements in Table~\\ref{tab1} the \n",
      "tab2\n",
      "Table~\\ref{tab2} lists the various couplings \n",
      "quadmo\n",
      "In Tables~\\ref{quadmo} and ~\\ref{transquad}\n",
      "Tables~\\ref{quadmo} and~\\ref{transquad}, it is straightforward to verify \n",
      "Tables~\\ref{quadmo} and~\\ref{transquad} can be written down if desirable.\n",
      "Table~\\ref{quadmo} with $B=r_n^2/4$ and $C=0$. \n",
      "transquad\n",
      "In Tables~\\ref{quadmo} and ~\\ref{transquad}\n",
      "Tables~\\ref{quadmo} and~\\ref{transquad}, it is straightforward to verify \n",
      "Tables~\\ref{quadmo} and~\\ref{transquad} can be written down if desirable.\n",
      "to the analytic expressions in Table~\\ref{transquad}.} \n",
      "quadmonum\n",
      "are listed in Tables~\\ref{quadmonum} and \\ref{transquadnum} for the cases\n",
      "transquadnum\n",
      "are listed in Tables~\\ref{quadmonum} and \\ref{transquadnum} for the cases\n",
      "octumom\n",
      "In the second column of Table~\\ref{octumom} \n",
      "as shown in the third column of Table~\\ref{octumom}.\n",
      "Table~\\ref{octumom} it is straightforward to verify \n",
      "Table~\\ref{octumom} can be written down if desirable.\n",
      "using Eq.(\\ref{three}) and the expressions in Table~\\ref{octumom}.\n",
      "using Table~\\ref{octumom} with $C=-0.003$. Second column: SU(3) \n",
      "octumomnum\n",
      "These are listed in Table~\\ref{octumomnum}.\n",
      "8 tables found.\n",
      "10 figures found.\n",
      "\n",
      "\n",
      "2001.00204v3\n",
      "Document header could not be identified.\n",
      "\n",
      "\n",
      "2001.00205v3\n",
      "0 tables found.\n",
      "4 figures found.\n",
      "\n",
      "\n",
      "2001.00207v1\n",
      "File not found: Data_Drive - [Errno 2] No such file or directory: 'Data_Drive'\n",
      "0 tables found.\n",
      "8 figures found.\n",
      "\n",
      "\n",
      "2001.00208v2\n",
      "tbl:u_pipo_com\n",
      "After going through one or more convolutional layers, the features are fused together to have hierarchical structural information. The input feature maps to each level of U-Net and PIPO-FAN are listed in Table~\\ref{tbl:u_pipo_com} for comparison. In Table~\\ref{tbl:u_pipo_com}, $I_s$ denotes the input at the $s$-th scale, $f_{s,j}$ denotes the input features of the $j$-th convolution block at the $s$-th scale, and $S$ is the scale number of inputs. A feature map to the first convolutional block at each scale $f_{s,1}$ is set as $I_s$.\n",
      "tab:uni_val\n",
      "In Table~\\ref{tab:uni_val}, different combinations of datasets are used to train the multi-organ segmentation model. The proposed PIPO-FAN is used as the segmentation model. Dice score is used as the evaluation criterion. When training on the BTCV dataset, the model serves as the baseline of multi-organ segmentation. Three partially labeled datasets, LiTS, KiTS and Spleen segmentation datasets are individually added as additional training dataset to enhance training procedure.\n",
      "tab:uni_net\n",
      "The segmentation results on BTCV are shown in Table~\\ref{tab:uni_net}.\n",
      "tab:data_net_com2\n",
      "For comparison on partially labeled multi-organ segmentation, the results on the combined all datasets are shown in Table~\\ref{tab:data_net_com2}. PIPO-FAN outperforms all other networks on multi-organ segmentation in both the BTCV dataset and the combined all datasets. Some example results on LiTS are shown in Fig.~\\ref{fig:seg_examples2}. \n",
      "tab:2D_nets\n",
      "The five-fold cross validation results are shown in Table~\\ref{tab:2D_nets}.\n",
      "tab:val\n",
      "We first evaluated the effectiveness of the proposed equal convolutional depth (ECD) mechanism presented in Section~\\ref{sec:ECD}. Table~\\ref{tab:val} shows the the segmentation performance under different network configurations. As expected, using PIPO always outperforms single-scale input/output segmentation, which is indeed a ResU-Net. With PIPO to explore multi-scale image information, using ECD results in consistent performance improvement. It is worth noting that using ECD with only deep pyramid supervision (DPS) performs better than using AF without ECD. This not only shows that ECD is effective in extracting image features, but also illustrates the necessity of having good features for adaptive fusion module to work efficiently.\n",
      "tab:test\n",
      "We also tried different number of scales for the input and output to evaluate the relationship between scales and model capacity. The results are shown in Table~\\ref{tab:test}. It can be seen that the larger scale numbers of input scales and output scales, the better segmentation accuracy the model obtains. It may be because the higher scales can provide larger receptive fields and thus enhance the contextual information, This in turn helps the feature abstraction, i.e. extracting representative segmentation features. A special case is that when the number of input scale increased from 3 to 5, but the number of output scale remains at 1, the segmentation performance dropped. It may be because increasing the input scale alone without additional output supervision adds difficulty to the network training.  \n",
      "tab:comparison\n",
      "However, such two-step methods can be computationally expensive. For example, the method in \\cite{li_hdenseu} takes 21 hours to finetune a pretrained 2D DenseUNet and another 9 hours to finetune the H-DenseUNet with two Titan Xp GPUs. In contrast, our proposed method can be trained on a single Titan Xp GPU in 3 hours. More importantly, when segmenting a CT volume, our method only takes 0.04s for one slice on a single GPU, which is, to the best of our knowledge, the fastest segmentation method compared to other reported methods. In the same, we are able to obtain the same Dice performance and even better symmetric surface distance (SSD) (ASSD: 1.413 $<$ 1.450, MSSD: 24.408 $<$ 27.118, 2.421 $<$ 3.150). Table~\\ref{tab:comparison} shows the performance comparison with other published state-of-the-art methods on LiTS challenge dataset. Despite its simplicity, our proposed 2D network segments the liver in a single step and can obtain a very competitive performance with less than 0.2\\% drop in Dice, compared to the top performing method -- DeepX \\cite{yuan2017hierarchical}. \n",
      "8 tables found.\n",
      "6 figures found.\n",
      "\n",
      "\n",
      "2001.00211v1\n",
      "tbl\n",
      "1 tables found.\n",
      "0 figures found.\n",
      "\n",
      "\n",
      "2001.00212v1\n",
      "0 tables found.\n",
      "13 figures found.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Save paper name and authors as metadata\n",
    "csvfile_table = open(table_output_dir + table_result_file, 'w', newline='', encoding=\"utf-8\")\n",
    "spamwriter_table = csv.writer(csvfile_table, delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "csvfile_figure = open(figure_output_dir + figure_result_file, 'w', newline='', encoding=\"utf-8\")\n",
    "spamwriter_figure = csv.writer(csvfile_figure, delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "\n",
    "for paper in os.listdir(source_dir):\n",
    "    paper_path = source_dir + paper\n",
    "    if os.path.isdir(paper_path):\n",
    "        print(\"\\n\\n\" + paper)\n",
    "        tex_files = [x for x in os.listdir(paper_path) if x.endswith('.tex')]\n",
    "\n",
    "        complete_tex = \"\"\n",
    "        for file in tex_files:\n",
    "            try:\n",
    "                f = open(paper_path + \"/\" + file, \"r\", encoding=\"utf8\")\n",
    "                complete_tex += f.read()\n",
    "                f.close()\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(\"UnicodeDecodeError occurred. File could not be loaded.\")\n",
    "                continue\n",
    "\n",
    "        found_document_header = re.search(r\"\\\\documentclass\\[.*\\\\begin\\{document\\}\", complete_tex, re.DOTALL)\n",
    "        paper_id = str(uuid.uuid4())\n",
    "        if found_document_header:\n",
    "            paper_header_file = table_header_dir + paper_id + \".txt\"\n",
    "            f = open(paper_header_file, \"w\", encoding=\"utf-8\")\n",
    "            f.write(found_document_header.group(0))\n",
    "            f.close()\n",
    "        else:\n",
    "            print(\"Document header could not be identified.\")\n",
    "            continue\n",
    "\n",
    "        found_tables = re.findall(r\"\\\\begin\\{table\\*?\\}.*?\\\\end\\{table\\*?\\}\", complete_tex, re.DOTALL)\n",
    "        found_figures = re.findall(r\"\\\\begin\\{figure\\*?\\}.*?\\\\end\\{figure\\*?\\}\", complete_tex, re.DOTALL)\n",
    "\n",
    "        for table in found_tables:\n",
    "            #r\"\\\\caption\\{(.*?)}\"\n",
    "            #caption_match = re.search(r\"\\\\caption\\{(([^{}]*(\\{[^{}]*\\})?[^{}]*)+)\\}\", table)\n",
    "            caption_match = re.search(r\"\\\\caption\\{(.*?)}\", table)\n",
    "            label_match = re.search(r\"\\\\label\\{(.*?)\\}\", table)\n",
    "            if label_match:\n",
    "                found_label = label_match.group(1)\n",
    "                print(found_label)\n",
    "                found_paragraphs = re.findall(fr\".*\\\\ref\\{{{re.escape(found_label)}}}.*\", complete_tex)\n",
    "                for paragraph in found_paragraphs:\n",
    "                    print(paragraph)\n",
    "            if caption_match:\n",
    "                found_caption = caption_match.group(1)\n",
    "                table_id = str(uuid.uuid4())\n",
    "\n",
    "                spamwriter_table.writerow([table_id, paper_id, found_caption])\n",
    "                table_file_path = table_code_dir + table_id + \".txt\"\n",
    "                f = open(table_file_path, \"w\", encoding=\"utf-8\")\n",
    "                f.write(table)\n",
    "                f.close()\n",
    "\n",
    "        for figure in found_figures:\n",
    "            found_graphics = re.findall(r\"\\\\includegraphics(\\[.*?\\])*\\{(.*?)\\}\", figure)\n",
    "            #caption_match = re.search(r\"\\\\caption\\{(([^{}]*(\\{[^{}]*\\})?[^{}]*)+)\\}\", figure)\n",
    "            caption_match = re.search(r\"\\\\caption\\{(.*?)}\", figure)\n",
    "            if caption_match:\n",
    "                found_caption = caption_match.group(1)\n",
    "                for graphic in found_graphics:\n",
    "                    figure_id = str(uuid.uuid4())\n",
    "                    \n",
    "                    graphic_path = graphic[1]\n",
    "                    file_type = os.path.splitext(graphic_path)[-1]\n",
    "                    if os.path.isfile(paper_path+\"/\"+graphic_path):\n",
    "                        graphic_path = paper_path+\"/\"+graphic_path\n",
    "                    else:\n",
    "                        for ext in possible_extensions:\n",
    "                            possible_path = paper_path+\"/\"+graphic_path + ext\n",
    "                            if os.path.isfile(possible_path):\n",
    "                                graphic_path = possible_path\n",
    "                                break\n",
    "                                \n",
    "                    try:\n",
    "                        shutil.copy(graphic_path, figure_output_dir+figure_id+file_type)\n",
    "                        spamwriter_figure.writerow([figure_id, paper_id, found_caption])\n",
    "                    except FileNotFoundError as e:\n",
    "                        print(f\"File not found: {graphic_path} - {e}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "        print(f\"{len(found_tables)} tables found.\")\n",
    "        print(f\"{len(found_figures)} figures found.\")\n",
    "    #break\n",
    "    \n",
    "csvfile_table.close()\n",
    "csvfile_figure.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvfile_table.close()\n",
    "csvfile_figure.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53cb565",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
