{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5ead797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0da896d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131072"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# File and Directory Paths\n",
    "source_dir = \"source_files/\"\n",
    "source_metadata_file = source_dir + \"papers.csv\"\n",
    "processed_metadata_file = source_dir + \"papers_processed.csv\"\n",
    "\n",
    "table_output_dir = \"extracted_tables/\"\n",
    "table_code_dir = table_output_dir + \"table_code/\"\n",
    "table_images_dir = table_output_dir + \"table_image/\"\n",
    "table_metadata_file = table_output_dir + \"tables.csv\"\n",
    "\n",
    "figure_output_dir = \"extracted_figures/\"\n",
    "figure_metadata_file = figure_output_dir + \"figures.csv\"\n",
    "\n",
    "# CSV Size Limit\n",
    "csv.field_size_limit(260000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f603cf92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for original paper metadata file complete. 0 rows were corrupted.\n",
      "\n",
      "\n",
      "Check for processed paper metadata file complete. 0 rows were corrupted.\n"
     ]
    }
   ],
   "source": [
    "# Check paper metadata files for errors\n",
    "paper_original_row_length = 3\n",
    "original_corrupted_papers = []\n",
    "with open(source_metadata_file, \"r\", encoding=\"utf-8\") as paper_metadata:\n",
    "    spamreader = csv.reader(paper_metadata, delimiter=';', quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "    for row in spamreader:\n",
    "        if len(row) != paper_original_row_length:\n",
    "            original_corrupted_papers.append(row)\n",
    "            print(f\"Unexpected row length for: {row}\")\n",
    "print(f\"Check for original paper metadata file complete. {len(original_corrupted_papers)} rows were corrupted.\\n\\n\")\n",
    "        \n",
    "paper_processed_row_length = 5\n",
    "processed_corrupted_papers = []\n",
    "with open(processed_metadata_file, \"r\", encoding=\"utf-8\") as paper_metadata:\n",
    "    spamreader = csv.reader(paper_metadata, delimiter=\";\", quotechar='\"', quoting=csv.QUOTE_ALL)\n",
    "    for row in spamreader:\n",
    "        if len(row) != paper_processed_row_length:\n",
    "            processed_corrupted_papers.append(row)\n",
    "            print(f\"Unexpected row length for: {row}\")\n",
    "print(f\"Check for processed paper metadata file complete. {len(processed_corrupted_papers)} rows were corrupted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4480c466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 rows were fixed.\n"
     ]
    }
   ],
   "source": [
    "# Repairing paper titels including a semicolumn\n",
    "repaired_papers = {}\n",
    "for paper_row in original_corrupted_papers:\n",
    "    if len(paper_row) == 4:\n",
    "        new_row = [paper_row[0], paper_row[1] + \",\" + paper_row[2], paper_row[3]]\n",
    "        repaired_papers[paper_row[0]] = new_row\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected row length for {paper_row}.\")\n",
    "\n",
    "print(f\"{len(repaired_papers)} rows were fixed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d07bc333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original papers csv file was successfully overwritten.\n",
      "Processed papers csv file was successfully overwritten.\n"
     ]
    }
   ],
   "source": [
    "# Fix for original source paper file\n",
    "with open(source_metadata_file, \"r\", newline='', encoding='utf-8') as input_file:\n",
    "        with open(source_dir + \"tmp.csv\", \"w\", newline='', encoding='utf-8') as output_file:\n",
    "            csv_reader = csv.reader(input_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            csv_writer = csv.writer(output_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            \n",
    "            for row in csv_reader:\n",
    "                if row[0] in repaired_papers:\n",
    "                    csv_writer.writerow(repaired_papers[row[0]])\n",
    "                else:\n",
    "                    csv_writer.writerow(row)\n",
    "                    \n",
    "# Replace old csv file with new csv file\n",
    "os.replace(source_dir + \"tmp.csv\", source_metadata_file)\n",
    "print(\"Original papers csv file was successfully overwritten.\")\n",
    "\n",
    "# Fix for processed paper file\n",
    "with open(processed_metadata_file, \"r\", newline='', encoding='utf-8') as input_file:\n",
    "        with open(source_dir + \"tmp.csv\", \"w\", newline='', encoding='utf-8') as output_file:\n",
    "            csv_reader = csv.reader(input_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            csv_writer = csv.writer(output_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            \n",
    "            for row in csv_reader:\n",
    "                if row[0] in repaired_papers:\n",
    "                    if len(row) > paper_processed_row_length:\n",
    "                        new_row = repaired_papers[row[0]] + [row[-2], row[-1]]\n",
    "                        csv_writer.writerow(new_row)\n",
    "                    else:\n",
    "                        print(f\"Critical error for {row}. Please fix it manually.\")\n",
    "                        csv_writer.writerow(row)\n",
    "                else:\n",
    "                    csv_writer.writerow(row)\n",
    "                    \n",
    "# Replace old csv file with new csv file\n",
    "os.replace(source_dir + \"tmp.csv\", processed_metadata_file)\n",
    "\n",
    "print(\"Processed papers csv file was successfully overwritten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5636af9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 figures and 6 were removed.\n"
     ]
    }
   ],
   "source": [
    "# Checking if unprocessed ppaers are still on the disk\n",
    "for paper_row in processed_corrupted_papers:\n",
    "    if len(paper_row) != 3:\n",
    "        print(paper_row)\n",
    "    else:\n",
    "        paper_id = paper_row[0]\n",
    "        if not os.path.isdir(source_dir+paper_id):\n",
    "            raise ValueError(f\"No files found for paper {paper_id}\")\n",
    "            \n",
    "paper_ids = [row[0] for row in processed_corrupted_papers]\n",
    "critical_figs = []\n",
    "critical_tabs = []\n",
    "\n",
    "# Remove critical figures to allow for reprocessing\n",
    "with open(figure_metadata_file, \"r\", newline='', encoding='utf-8') as input_file:\n",
    "        with open(figure_output_dir + \"tmp.csv\", \"w\", newline='', encoding='utf-8') as output_file:\n",
    "            csv_reader = csv.reader(input_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            csv_writer = csv.writer(output_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            \n",
    "            for row in csv_reader:\n",
    "                if row[1] not in paper_ids:\n",
    "                    csv_writer.writerow(row)\n",
    "                else:\n",
    "                    critical_figs.append(row[0])\n",
    "os.replace(figure_output_dir + \"tmp.csv\", figure_metadata_file)\n",
    "\n",
    "# Remove critical figures to allow for reprocessing\n",
    "with open(table_metadata_file, \"r\", newline='', encoding='utf-8') as input_file:\n",
    "        with open(table_output_dir + \"tmp.csv\", \"w\", newline='', encoding='utf-8') as output_file:\n",
    "            csv_reader = csv.reader(input_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            csv_writer = csv.writer(output_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            \n",
    "            for row in csv_reader:\n",
    "                if row[1] not in paper_ids:\n",
    "                    csv_writer.writerow(row)\n",
    "                else:\n",
    "                    critical_tabs.append(row[0])\n",
    "os.replace(table_output_dir + \"tmp.csv\", table_metadata_file)\n",
    "\n",
    "print(f\"{len(critical_figs)} figures and {len(critical_tabs)} have been removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "07396c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed papers csv file was successfully overwritten.\n"
     ]
    }
   ],
   "source": [
    "# Removing these unprocessed papers from csv file\n",
    "with open(processed_metadata_file, \"r\", newline='', encoding='utf-8') as input_file:\n",
    "        with open(source_dir + \"tmp.csv\", \"w\", newline='', encoding='utf-8') as output_file:\n",
    "            csv_reader = csv.reader(input_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            csv_writer = csv.writer(output_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            \n",
    "            for row in csv_reader:\n",
    "                if row[0] not in paper_ids:\n",
    "                    csv_writer.writerow(row)\n",
    "                    \n",
    "# Replace old csv file with new csv file\n",
    "os.replace(source_dir + \"tmp.csv\", processed_metadata_file)\n",
    "\n",
    "print(\"Processed papers csv file was successfully overwritten.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "944662cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for source_files/papers.csv completed.\n",
      "Check for source_files/papers_processed.csv completed.\n",
      "Check for extracted_figures/figures.csv completed.\n",
      "Check for extracted_tables/tables.csv completed.\n"
     ]
    }
   ],
   "source": [
    "# Checking metadata files for double occurrences\n",
    "def check_for_double(file_path):\n",
    "    with open(file_path, \"r\", newline='', encoding='utf-8') as csv_file:\n",
    "        spamreader = csv.reader(csv_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        id_set = set()\n",
    "        for row in spamreader:\n",
    "            if row[0] in id_set:\n",
    "                print(f\"Double finding of {row[0]}\")\n",
    "            id_set.add(row[0])\n",
    "    print(f\"Check for {file_path} completed.\")\n",
    "    \n",
    "check_for_double(source_metadata_file)\n",
    "check_for_double(processed_metadata_file)\n",
    "check_for_double(figure_metadata_file)\n",
    "check_for_double(table_metadata_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2932e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
