{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fefb9f3-2e49-4ddb-a85e-4a2988cb24b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b05f38-0713-4fae-9728-50db442c78cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful loaded.\n"
     ]
    }
   ],
   "source": [
    "# Variables and directories\n",
    "workspace_dir = \"/pfs/work7/workspace/scratch/ma_frajwa-dataset/\"\n",
    "qa_output_dir = workspace_dir + \"qa_output/\"\n",
    "\n",
    "table_dir = workspace_dir + \"extracted_tables/\"\n",
    "table_image_dir = table_dir + \"table_images/\"\n",
    "table_code_dir = table_dir + \"table_code/\"\n",
    "table_metadata = table_dir + \"tables.csv\"\n",
    "\n",
    "figure_dir = workspace_dir + \"extracted_figures/\"\n",
    "figure_metadata = figure_dir + \"figures.csv\"\n",
    "\n",
    "os.makedirs(qa_output_dir, exist_ok=True)\n",
    "\n",
    "if os.path.isfile(table_metadata):\n",
    "    print(\"Successful loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3735f261-dad5-4cbe-b0e7-5026592e4ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define QA-Pair-Generation Prompts\n",
    "table_prompt = \"\"\"\n",
    "Table: {table_code} \n",
    "Caption: {caption} \n",
    "Text mentions: {text_mentions} \n",
    "Question: [Your generated question here] \n",
    "Answer: [Your generated answer here]\"\"\"\n",
    "\n",
    "figure_prompt = \"\"\"\n",
    "Caption: {caption} \n",
    "Text mentions: {text_mentions} \n",
    "Question: [Your generated question here] \n",
    "Answer: [Your generated answer here]\"\"\"\n",
    "\n",
    "few_shot_learning_table = \"\"\"\n",
    "Generate an open-ended question and its corresponding answer based on a scientific table. Use its caption and text mentions\n",
    "from the scientific paper to create a question that tests the understanding of this specific table and answer it afterwards. \n",
    "The answer should be unambigious and consist of as few words as possible.\n",
    "\n",
    "Caption: Experimental results on the ISIDCM-20 dataset. ``Mean\" denotes the average CT value in the selected ROIs. ``AR\" denotes the absolute error between the average CT value of each method and that of referenced NDCT in the selected ROIs. The best and second-best performance in each column is colorized by the \\textcolor{red}{red} and the  \\textcolor{blue}{blue}.\n",
    "Text mentions: \\subsubsection{Performance comparison on clinical dataset} We follow \\cite{10081080} to choose the two most representative ROIs for performance comparison on clinical datasets, as shown in Figures \\ref{real1} and \\ref{real2}. We calculate the average CT Hounsfield units value and corresponding absolute error with reference images (unpaired NDCT images) as used in \\cite{10081080}.  As illustrated in Table \\ref{isicdm-20}, quantitative results demonstrate that our proposed method has a closer CT value with the reference image. For example, in ROI-2, the average CT value of our proposed method is -25.15, instead, the second-best result achieved by CCDnet is only -13.91. By observing visualized results in Figure \\ref{real1}, it seems that most baseline methods (especially for Noiser2noise and CycleGAN) achieve an unfavorable noise suppression for this very challenging LDCT image with complex and serious noise.\n",
    "Question: What is the second-best average CT value in ROI-2?\\\n",
    "Answer: -13.91.\n",
    "\n",
    "Caption: \\label{tab:pkg-times} Comparison of different \\proglang{Python} packages for Bayesian optimization. The elapsed time per iteration averaged across the ten runs is given for each package.\n",
    "Text mentions: However, the time \\pkg{NUBO} requires to complete one iteration with a maximum of 2.20s for D) is, on average, higher than for the other packages (Table~\\ref{tab:pkg-times}). While this might be important for some areas of optimization, it is negligible when optimizing expensive black-box functions, as these functions are much more resource-intensive to evaluate. Thus, the small number of additional seconds that \\pkg{NUBO} requires per iteration is insignificant compared to the resources required to conduct an experiment or a simulation.\n",
    "Question: Which package has the highest elapsed time per iteration in scenario D, and what is its maximum recorded time?\n",
    "Answer: NUBO; 2.20s.\n",
    "\n",
    "Caption: Change area estimates for Tigray region and each zone within Tigray (units are kha)\n",
    "Text Mentions: Table \\ref{tab:change-area} summarizes the area estimates for the four transition classes. We found that cropland gain accounted for $70-160$ kha while cropland loss accounted for $19-79$ kha in Tigray. The stable transition classes accounted for the majority of the area in Tigray: $1,033-1,293$ kha of stable cropland and $3,798-4,054$ kha of stable non-cropland.\n",
    "Question: What is the estimated range of area for stable cropland in the Tigray region?\n",
    "Answer: 1,033-1,293 kha.\n",
    "\n",
    "Caption: \\label{tab:heterogeneity}Heterogeneity Analysis \n",
    "Text Mentions: Table \\ref{tab:heterogeneity} presents the results of a heterogeneity analysis using the sample of users with an organisational affiliation. The results reveal that the adverse effects of the ban on productivity are mainly driven by users who created their profile prior to 2016 and users with 15 followers or less. While the latter could be an indicator for less skilled users, the former results suggests that older users are more impacted by the ban.\n",
    "Question: Which two user groups are primarily driving the adverse effects of the ban on productivity?\n",
    "Answer: Users who created their profile prior to 2016 and users with 15 followers or less.\n",
    "\n",
    "Caption: Experiment 2 Results: Summary of the median, mean, and standard deviation for the epochs required to reach the threshold value across 1000 simulation runs, and a counter of faster convergence runs between models.\n",
    "Text Mentions: The principal findings of the second experiment are presented in Table~\\ref{tab:Iterations}, providing a comprehensive comparison of the epochs required to achieve the predefined loss value of $0.05$, as outlined in Section~\\ref{setup2}, between the SCQRNN model and the CQRNN baseline model.\n",
    "Question: Which predefined loss value is used as the threshold for comparing the epochs required for convergence between the SCQRNN and CQRNN models?\n",
    "Answer: 0.05.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "few_shot_learning_figure = \"\"\"\n",
    "Generate an open-ended question and its corresponding answer based on a scientific figure. Use its caption and text mentions\n",
    "from the scientific paper to create a question that tests the understanding of this specific figure and answer it afterwards. \n",
    "The answer should be unambigious and consist of as few words as possible.\n",
    "\n",
    "2209.01769_FIG_4\n",
    "Caption: \\textcolor{black}{Rate-distortion comparison of GOP sizes 4, 8, 16 on UVG dataset under intra-period 32.} \n",
    "Text mentions: We evaluate the effect of GOP size on the performance of B-CANF. A number of GOP sizes, including 4, 8, 16, are tested with intra-period 32. The BD-rates are summarized in Table~\\ref{tab:abl_gop} (see the results w/o a separate P-frame codec). The corresponding rate-distortion curves on UVG dataset are presented in Fig.~\\ref{fig:abl_gop_size}. From Fig.~\\ref{fig:abl_gop_size}, the rate-distortion performance of B-CANF is seen to improve with the increased GOP size. The improvement is most obvious at low rates. Like P-frames, our B*-frames suffer more from temporal error propagation with smaller GOP sizes (in which cases, B*-frames are sent more frequently), especially at low rates where poor reconstruction and motion quality is expected. Increasing GOP size decreases the frequency of B*-frames, thereby reducing temporal error propagation.\n",
    "Question: How does increasing the GOP size affect the rate-distortion performance of B-CANF on the UVG dataset under intra-period 32?\n",
    "Answer: Increasing GOP size improves rate-distortion performance.\n",
    "\n",
    "2203.08550_FIG_7\n",
    "Caption: The finger contribution comparison of the bending angle for human and robot hands. (a) Single-direction grasp, (b) Bidirectional grasp.\n",
    "Text Mentions: Based on the collected data of the fingers of the soft robot and human hands, the proportion of the bending angle were calculated to analyze the contribution of each fingers. As shown in Figure \\ref{IROSFigBiomimeticRatioComparsion}, for the single-direction grasp, the thumb, index and middle fingers act as the main roles for the grasp pose. Beside, the relative high weight of the human ring finger is caused by the [missing part]\n",
    "Question: Which fingers contribute the most to the bending angle in a single-direction grasp for both human and robot hands?\n",
    "Answer: Thumb, index, and middle fingers.\n",
    "\n",
    "2206.07171_FIG_2\n",
    "Caption: Categorization of the 38 papers reviewed in this survey. The papers are first categorized on the learning paradigm (fully vs. semi/un/self-supervised) and on the segmentation type (semantic vs. instance). Each quadrant shows the distributions of applications (2D vs. 3D) and DL backbones (U-Net vs. FCN vs. Other) of the papers that use the corresponding learning and segmentation approaches. \n",
    "Text mentions: Fig.~\\ref{fig:SearchResultSummary} summarizes this collection of 38 papers in terms of learning technique (fully supervised or not), segmentation type (semantic or instance), application (2D or 3D) and the underlying modeling backbone. Before reviewing these papers, we discuss the key EM datasets and describe the evolution of DL architectures, which are two crucial components that have been permitting the progress of EM segmentation analysis.\n",
    "Question: How are the 38 papers reviewed in the survey categorized?\n",
    "Answer: The papers are categorized by learning paradigm (fully vs. semi/un/self-supervised) and segmentation type (semantic vs. instance).\n",
    "\n",
    "2201.06313_FIG_4\n",
    "Caption: The general architecture of the proposed method \n",
    "Text mentions: In designing the model for the proposed method, the SoftMax function must be used in the output layer of the model because each category has three different classes of positive, negative, and neutral. Since our number of categories is 9, 9 SoftMax functions with three neurons were used. Fig. \\ref{fig:4} shows the general architecture of the proposed method based on hard parameter sharing to solve the two sub-tasks of aspect category detection and aspect category polarity for joint learning.\n",
    "Question: How is the output layer of the general architecture of the proposed method designed?\n",
    "Answer: The output layer uses 9 SoftMax functions with three neurons each for the categories: positive, negative, and neutral.\n",
    "\n",
    "2205.10981_FIG_5\n",
    "Caption: GPT-3 Classification Endpoint mean performance with standard errors on data science question topic classification. Training data is augmented by adding different quantities of new examples generated with GPT-3 Davinci Completion. \n",
    "Text mentions: While, for the validation set, accuracy was positively related to the number of questions generated, the same was not true for the test set. Figure \\ref{res:fig-class} plots the relationship between accuracy and number of example questions added across both validation and test sets, with the shaded regions representing the standard error (68 percent confidence interval). Note that the x-axis is a log scale. On the test set, accuracy scarcely increased at all until the number of questions added reached about 1,000, at which point it increased to 76 percent. This represented peak accuracy; augmented training sets with 10,000 new questions averaged only 73 percent accuracy, a slight drop.\n",
    "Question: How does augmenting training data with GPT-3-generated examples affect test set accuracy in data science question topic classification?\n",
    "Answer: Test set accuracy increases slightly after adding about 1,000 examples, peaking at 76%, but decreases slightly to 73% with 10,000 examples.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "354da044-b23f-4f26-9c74-fabef050056a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a qa_pair, either for a figure or a table\n",
    "def generate_qa_pair(object_id, image_file, caption, text_mentions, table_code=None):\n",
    "    # Modifying the prompt\n",
    "    task_input_prompt = None\n",
    "    instruction_prompt = None\n",
    "    if table_code:\n",
    "        instruction_prompt = few_shot_learning_table\n",
    "        task_input_prompt = table_prompt.replace(\"{caption}\", caption).replace(\"{text_mentions}\", text_mentions).replace(\"{table_code}\", table_code)\n",
    "    else:\n",
    "        instruction_prompt = few_shot_learning_figure\n",
    "        task_input_prompt = figure_prompt.replace(\"{caption}\", caption).replace(\"{text_mentions}\", text_mentions)\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant that can understand and generate question-answer pairs from scientific data.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{instruction_prompt}\\n{task_input_prompt}\"}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=512\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    # Receiving the results and store them in file\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdb3b258-75de-4bd5-9c12-543d412d14f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read either figure or table data from csv file\n",
    "def get_object_data(meta_file, start_index, end_index, table=False):\n",
    "    object_data = {}\n",
    "    with open(meta_file, \"r\", newline='', encoding='utf-8') as csv_file:\n",
    "        spamreader = csv.reader(csv_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "        index = 0\n",
    "        for row in spamreader:\n",
    "            if index >= start_index:\n",
    "                object_id = row[0]\n",
    "                caption = row[3]\n",
    "                text_mentions = row[4]\n",
    "\n",
    "                if table: # For tables\n",
    "                    try:\n",
    "                        table_code = get_table_code(table_code_dir + object_id + \".tex\")\n",
    "                        object_data[object_id] = (caption, text_mentions, table_code)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error occurred for index {index}: {e}\")\n",
    "                else: # For figures\n",
    "                    object_data[object_id] = (caption, text_mentions)\n",
    "            index += 1\n",
    "            if index > end_index:\n",
    "                break\n",
    "\n",
    "    return object_data\n",
    "\n",
    "# Return table code\n",
    "def get_table_code(code_file):\n",
    "    table_code = None\n",
    "    if os.path.isfile(code_file):\n",
    "        with open(code_file, \"r\", encoding='utf-8') as code_file:\n",
    "            table_code = code_file.read()\n",
    "            splitted_code = table_code.split(\"\\pagenumbering{gobble}\")\n",
    "            if len(splitted_code) != 2:\n",
    "                raise ValueError(f\"Unexpected occurrence of pagenumbering. Please check manually {code_file}\")\n",
    "            table_code = splitted_code[-1]\n",
    "            table_code = table_code.replace(\"\\end{document}\", \"\")\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"{code_file} was not found.\")\n",
    "    return table_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17dcf78e-63c9-4c3b-8ce4-3dd4a9574646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute whole QA generation for either figures or tables, following a range of indexes in the metadata file\n",
    "def execute_generation(start_index, end_index, table=False):\n",
    "    print(\"Data extraction from csv file started.\")\n",
    "    object_data = None\n",
    "    try:\n",
    "        if table:\n",
    "            object_data = get_object_data(table_metadata, start_index, end_index, True)\n",
    "        else:\n",
    "            object_data = get_object_data(figure_metadata, start_index, end_index)\n",
    "    except Exception as e:\n",
    "        print(\"Error during data extraction:\", e)\n",
    "        return None\n",
    "\n",
    "    print(\"QA-pair generation started.\")\n",
    "    counter = 0\n",
    "    for obj in object_data:\n",
    "        image_file = None\n",
    "        table_code = None\n",
    "        caption = object_data[obj][0]\n",
    "        text_mentions = object_data[obj][1]\n",
    "        if table:\n",
    "            image_file = table_image_dir + obj + \".png\"\n",
    "            table_code = object_data[obj][2]\n",
    "        else:\n",
    "            image_file = figure_image_dir + obj + \".png\"\n",
    "            \n",
    "        # Generation\n",
    "        try:    \n",
    "            response = generate_qa_pair(obj, image_file, caption, text_mentions, table_code)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        else:\n",
    "            output_file = qa_output_dir + obj + \".txt\"\n",
    "            with open(output_file, \"w\", encoding='utf-8') as output:\n",
    "                output.write(response)\n",
    "\n",
    "        counter += 1\n",
    "        if counter % int(len(object_data)/10) == 0:\n",
    "            print(f\"{counter} objects have been processed.\")\n",
    "\n",
    "    print(\"Process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13686684-c9bd-4447-b435-c08d86db39bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 00:08:18.335786: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-03 00:08:18.692124: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1735859299.040900  421948 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1735859299.611566  421948 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-03 00:08:21.259675: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "212d8bc5e8504b6a8aec3bface2f74a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model and tokenizer\n",
    "model_id = \"Qwen/Qwen2.5-14B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01905128-6507-4e30-b3e2-8f0180a9e40f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction from csv file started.\n",
      "Error occurred for index 905: Unexpected occurrence of pagenumbering. Please check manually <_io.TextIOWrapper name='/pfs/work7/workspace/scratch/ma_frajwa-dataset/extracted_tables/table_code/2406.10356_TAB_1.tex' mode='r' encoding='utf-8'>\n",
      "QA-pair generation started.\n",
      "1 objects have been processed.\n",
      "2 objects have been processed.\n",
      "CUDA out of memory. Tried to allocate 870.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 99.62 MiB is free. Including non-PyTorch memory, this process has 31.62 GiB memory in use. Of the allocated memory 30.35 GiB is allocated by PyTorch, and 919.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "3 objects have been processed.\n",
      "4 objects have been processed.\n",
      "5 objects have been processed.\n",
      "6 objects have been processed.\n",
      "7 objects have been processed.\n",
      "8 objects have been processed.\n",
      "9 objects have been processed.\n",
      "10 objects have been processed.\n",
      "11 objects have been processed.\n",
      "CUDA out of memory. Tried to allocate 1.35 GiB. GPU 0 has a total capacity of 31.73 GiB of which 1.31 GiB is free. Including non-PyTorch memory, this process has 30.41 GiB memory in use. Of the allocated memory 29.44 GiB is allocated by PyTorch, and 619.64 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "12 objects have been processed.\n",
      "13 objects have been processed.\n",
      "14 objects have been processed.\n",
      "15 objects have been processed.\n",
      "16 objects have been processed.\n",
      "17 objects have been processed.\n",
      "CUDA out of memory. Tried to allocate 950.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 389.62 MiB is free. Including non-PyTorch memory, this process has 31.33 GiB memory in use. Of the allocated memory 30.09 GiB is allocated by PyTorch, and 896.37 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "18 objects have been processed.\n",
      "19 objects have been processed.\n",
      "CUDA out of memory. Tried to allocate 1.12 GiB. GPU 0 has a total capacity of 31.73 GiB of which 185.62 MiB is free. Including non-PyTorch memory, this process has 31.53 GiB memory in use. Of the allocated memory 30.57 GiB is allocated by PyTorch, and 609.76 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "20 objects have been processed.\n",
      "CUDA out of memory. Tried to allocate 1.56 GiB. GPU 0 has a total capacity of 31.73 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 30.61 GiB memory in use. Of the allocated memory 29.67 GiB is allocated by PyTorch, and 585.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "21 objects have been processed.\n",
      "CUDA out of memory. Tried to allocate 1.51 GiB. GPU 0 has a total capacity of 31.73 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 30.61 GiB memory in use. Of the allocated memory 29.62 GiB is allocated by PyTorch, and 641.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "22 objects have been processed.\n",
      "CUDA out of memory. Tried to allocate 1.13 GiB. GPU 0 has a total capacity of 31.73 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 30.61 GiB memory in use. Of the allocated memory 29.17 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "23 objects have been processed.\n",
      "CUDA out of memory. Tried to allocate 894.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 235.62 MiB is free. Including non-PyTorch memory, this process has 31.48 GiB memory in use. Of the allocated memory 29.95 GiB is allocated by PyTorch, and 1.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "24 objects have been processed.\n",
      "25 objects have been processed.\n",
      "26 objects have been processed.\n",
      "27 objects have been processed.\n",
      "28 objects have been processed.\n",
      "29 objects have been processed.\n",
      "30 objects have been processed.\n",
      "CUDA out of memory. Tried to allocate 1.57 GiB. GPU 0 has a total capacity of 31.73 GiB of which 1.13 GiB is free. Including non-PyTorch memory, this process has 30.58 GiB memory in use. Of the allocated memory 29.69 GiB is allocated by PyTorch, and 543.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "31 objects have been processed.\n",
      "32 objects have been processed.\n",
      "33 objects have been processed.\n",
      "34 objects have been processed.\n",
      "35 objects have been processed.\n",
      "36 objects have been processed.\n",
      "37 objects have been processed.\n",
      "38 objects have been processed.\n",
      "39 objects have been processed.\n",
      "40 objects have been processed.\n",
      "CUDA out of memory. Tried to allocate 782.00 MiB. GPU 0 has a total capacity of 31.73 GiB of which 409.62 MiB is free. Including non-PyTorch memory, this process has 31.31 GiB memory in use. Of the allocated memory 30.12 GiB is allocated by PyTorch, and 851.26 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "41 objects have been processed.\n",
      "42 objects have been processed.\n",
      "43 objects have been processed.\n",
      "44 objects have been processed.\n"
     ]
    }
   ],
   "source": [
    "# Specify function parameters\n",
    "s_index = 53\n",
    "e_index = 1000\n",
    "is_table = True\n",
    "\n",
    "# Execute QA generation\n",
    "execute_generation(s_index, e_index, is_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55a35e3-9d88-4673-a90a-ba2084cb1dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goes through the qa_output_dir and locates the indices of the last table/figure created\n",
    "def find_last_added_qa_pair():\n",
    "    qa_pairs_table = set()\n",
    "    qa_pairs_figure = set()\n",
    "    \n",
    "    for file in os.listdir(qa_output_dir):\n",
    "        if \"TAB\" in file:\n",
    "            qa_pairs_table.add(file)\n",
    "        else:\n",
    "            qa_pairs_figure.add(file)\n",
    "            \n",
    "    if len(qa_pairs_table) > 0:\n",
    "        last_created_table = max(qa_pairs_table, key=os.path.getmtime).replace(\".txt\", \"\")\n",
    "        \n",
    "        with open(table_metadata, \"r\", newline='', encoding='utf-8') as csv_file:\n",
    "        spamreader = csv.reader(csv_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        counter = 0\n",
    "        for row in spamreader:\n",
    "            if row[0] == last_created_table:\n",
    "                print(f\"Last table index: {counter}\")\n",
    "                print(row, \"\\n\")\n",
    "                break\n",
    "            counter += 1\n",
    "        \n",
    "    if len(qa_pairs_figure) > 0:\n",
    "        last_created_figure = max(qa_pairs_figure, key=os.path.getmtime).replace(\".txt\", \"\")\n",
    "        \n",
    "        with open(figure_metadata, \"r\", newline='', encoding='utf-8') as csv_file:\n",
    "        spamreader = csv.reader(csv_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        counter = 0\n",
    "        for row in spamreader:\n",
    "            if row[0] == last_created_figure:\n",
    "                print(f\"Last figure index: {counter}\")\n",
    "                print(row, \"\\n\")\n",
    "                break\n",
    "            counter += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
