{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69506d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dea77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File and Directory Paths\n",
    "qa_pair_file = \"qa_pairs.csv\"\n",
    "qa_pair_filtered_file = \"qa_pairs_LLM_filtered.csv\"\n",
    "qa_pair_output = \"qa_pairs_final.csv\"\n",
    "\n",
    "# Defining column indexes\n",
    "object_id_index_original = 0\n",
    "object_type_index_original = 1\n",
    "question_index_original = 2\n",
    "\n",
    "question_index_filtered = 1\n",
    "valid_value_index_filtered = -1\n",
    "\n",
    "# Defining values\n",
    "INVALID_VALUE = \"Invalid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10f35e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of not unique questions: 630\n",
      "Number of affected pairs: 1260\n",
      "\n",
      "Examples for not unique questions:\n",
      "According to the IVQR estimates, at which quantiles are the effects of Chinese imports on wages most significant?                                                                                       2\n",
      "According to the simulations, in which scenario were TTW estimates more accurateâ€”when using a single instrument or multiple instruments for inference?                                                  2\n",
      "In which scenario does the model trained on CDJUR-BR achieve a higher macro F1 score when tested against LENER-BR compared to the reverse scenario?                                                     2\n",
      "According to the table, what abbreviation is used for the setting where strong augmentations ($\\tau_s$) and weak augmentations ($\\tau_t$) are applied along with shared student and teacher weights?    2\n",
      "What is the average cosine similarity (ACS) reported for each diagnosis in the table comparing COSMIC signatures to consensus signatures?                                                               2\n",
      "What elements' numerical results for $c_f$ and $\\phi$ are provided in Table \\ref{FEDG1}?                                                                                                                2\n",
      "According to the Bracket Flare test dataset, which method performed better than Flare7K++?                                                                                                              2\n",
      "What are the RTFs of the encoder and decoder for the proposed model on an iPhone XS?                                                                                                                    2\n",
      "Which frequency bands were designated by the 3GPP for NTN operation in the United States and internationally according to Table \\ref{tab_NTN_freq}?                                                     2\n",
      "How many entries show that level-balanced designs are inferior in terms of the $Q_B$ value compared to non-level balanced designs for fully estimable submodels?                                        2\n",
      "Name: 1, dtype: int64\n",
      "630 rows have been deleted due to multiple occurrences.\n"
     ]
    }
   ],
   "source": [
    "def find_not_unique_questions(file_path):\n",
    "    \"\"\"\n",
    "    Locating questions that are not unique, meaning they occur multiple times within the dataset.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the QA-pair csv file.\n",
    "        \n",
    "    Returns:\n",
    "        set[str]: A set of all non-unique questions.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, delimiter=\";\", quotechar=\"|\", header=None)\n",
    "    value_counts = df.iloc[:, question_index_filtered].value_counts()\n",
    "    not_unique_questions = value_counts[value_counts >= 2]\n",
    "    \n",
    "    print(f\"Number of not unique questions: {len(not_unique_questions)}\")\n",
    "    print(f\"Number of affected pairs: {not_unique_questions.sum()}\")\n",
    "    print(\"\\nExamples for not unique questions:\")\n",
    "    print(not_unique_questions[:10])\n",
    "    \n",
    "    question_set = set()\n",
    "    for question in not_unique_questions.index:\n",
    "        question_set.add(question)\n",
    "    return question_set\n",
    "\n",
    "def remove_not_unique_questions(file_path):\n",
    "    \"\"\"\n",
    "    This function removes the first occurrence of a not unique question from the dataset.\n",
    "    In this stage, this is needed when filter_questions_LLM.py was executed multiple times with wrong indexing.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the QA-pair csv file.\n",
    "    \"\"\"\n",
    "    not_unique_questions = find_not_unique_questions(file_path)\n",
    "    counter_removed = 0\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as input_file:\n",
    "        csv_reader = csv.reader(input_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        with open(\"tmp.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as output_file:\n",
    "            csv_writer = csv.writer(output_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            \n",
    "            for row in csv_reader:\n",
    "                question = row[question_index_filtered]\n",
    "                if question in not_unique_questions:\n",
    "                    not_unique_questions.remove(question)\n",
    "                    counter_removed += 1\n",
    "                else:\n",
    "                    csv_writer.writerow(row)\n",
    "    \n",
    "    # Replace old csv file with new csv file\n",
    "    os.replace(\"tmp.csv\", file_path)\n",
    "    print(f\"{counter_removed} rows have been deleted due to multiple occurrences.\")\n",
    "    \n",
    "remove_not_unique_questions(qa_pair_filtered_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0072ec52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid      87450\n",
      "Invalid    12565\n",
      "Name: 3, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def count_valid_invalid_pairs(file_path):\n",
    "    \"\"\"\n",
    "    This function prints the count of valid and invalid QA-pairs.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the QA-pair csv file filtered by an LLM.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(file_path, delimiter=\";\", quotechar=\"|\", header=None)\n",
    "    value_counts = df.iloc[:, valid_value_index_filtered].value_counts()\n",
    "    \n",
    "    print(value_counts)\n",
    "    \n",
    "count_valid_invalid_pairs(qa_pair_filtered_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33a47e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of deleted QA-pairs per category:\n",
      "Table: 3640 / 26823\n",
      "Figure: 5846 / 51186\n",
      "Table_02: 3079 / 22006\n"
     ]
    }
   ],
   "source": [
    "def get_invalid_pairs(file_path):\n",
    "    \"\"\"\n",
    "    This function returns all questions that were labelled 'invalid' by the LLM.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The path to the QA-pair csv file filtered by an LLM.\n",
    "        \n",
    "    Returns:\n",
    "        set[str]: A set of all questions that were labelled 'Invalid' by the LLM.\n",
    "    \"\"\"\n",
    "    # Filter dataframe\n",
    "    df = pd.read_csv(file_path, delimiter=\";\", quotechar=\"|\", header=None)\n",
    "    df = df[df.iloc[:, valid_value_index_filtered] == INVALID_VALUE]\n",
    "    df = df.iloc[:, question_index_filtered]\n",
    "    \n",
    "    # Create set\n",
    "    question_set = set()\n",
    "    for question in df:\n",
    "        question_set.add(question)\n",
    "        \n",
    "    return question_set\n",
    "\n",
    "def remove_invalid_pairs(original_file, output_file, invalid_pairs):\n",
    "    \"\"\"\n",
    "    This function creates a new csv file containing only questions labelled as 'Valid' by the LLM.\n",
    "    \n",
    "    Args:\n",
    "        original_file (str): The path to the original QA-pair csv file without any LLM-filtering.\n",
    "        output_file (str): The path to where the valid QA-pairs shall be stored.\n",
    "        invalid_pairs (set[str]): A set of all invalid questions.\n",
    "    \"\"\"\n",
    "    overall_dict = defaultdict(int)\n",
    "    removed_dict = defaultdict(int)\n",
    "    \n",
    "    # Iterate through original file\n",
    "    with open(original_file, \"r\", encoding=\"utf-8\") as input_file:\n",
    "        csv_reader = csv.reader(input_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        \n",
    "        with open(output_file, \"w\", newline=\"\", encoding=\"utf-8\") as new_file:\n",
    "            csv_writer = csv.writer(new_file, delimiter=';', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "            \n",
    "            for row in csv_reader:\n",
    "                overall_dict[row[object_type_index_original]] += 1\n",
    "                if row[question_index_original] in invalid_pairs:\n",
    "                    removed_dict[row[object_type_index_original]] += 1\n",
    "                else:\n",
    "                    csv_writer.writerow(row)\n",
    "                    \n",
    "    # Print results\n",
    "    print(\"Number of deleted QA-pairs per category:\")\n",
    "    for key in removed_dict:\n",
    "        print(f\"{key}: {removed_dict[key]} / {overall_dict[key]}\")\n",
    "    \n",
    "    \n",
    "invalid_pairs = get_invalid_pairs(qa_pair_filtered_file)\n",
    "remove_invalid_pairs(qa_pair_file, qa_pair_output, invalid_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c857e24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train and test splits were successfully constructed.\n"
     ]
    }
   ],
   "source": [
    "FIXED_FIGURE_SAMPLE_SIZE = 1100\n",
    "FIXED_TABLE_SAMPLE_SIZE = 550\n",
    "\n",
    "def perform_train_test_split(input_file, train_split_file, test_split_file, figure_sample_size, table_sample_size):\n",
    "    \"\"\"\n",
    "    Performing the train-test split.\n",
    "    \n",
    "    Args:\n",
    "        input_file (str): Path to the csv file containing the QA-pairs.\n",
    "        train_split_file (str): Path to the file in which the training split shall be stored.\n",
    "        test_split_file (str): Path to the file in which the test split shall be stored.\n",
    "        figure_sample_size (int): Number of figures that shall be present in the test split.\n",
    "        table_sample_size (int): Number of tables (per sub-category) that shall be present in the test split.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Loading csv file\n",
    "    df = pd.read_csv(input_file, delimiter=\";\", quotechar=\"|\", header=None)\n",
    "    df[object_type_index_original] = df[object_type_index_original].astype(str)\n",
    "    \n",
    "    # Test split per label\n",
    "    test_label1 = df[df[object_type_index_original] == \"Figure\"].sample(n=figure_sample_size, random_state=42)\n",
    "    test_label2 = df[df[object_type_index_original] == \"Table\"].sample(n=table_sample_size, random_state=42)\n",
    "    test_label3 = df[df[object_type_index_original] == \"Table_02\"].sample(n=table_sample_size, random_state=42)\n",
    "    \n",
    "    # Joining up test split\n",
    "    test_split = pd.concat([test_label1, test_label2, test_label3])\n",
    "    \n",
    "    # Move the rest to train split. Remove any rows that contain an object that also occurs in the test split.\n",
    "    train_split = df.drop(test_split.index)\n",
    "    train_split = train_split[~train_split[object_id_index_original].isin(test_split[object_id_index_original])]\n",
    "    \n",
    "    # Save splits to csv files\n",
    "    train_split.to_csv(train_split_file, index=False, header=False, sep=';', quotechar='|')\n",
    "    test_split.to_csv(test_split_file, index=False, header=False, sep=';', quotechar='|')\n",
    "    \n",
    "    # Confirmation print\n",
    "    print(\"Train and test splits were successfully constructed.\")\n",
    "    \n",
    "perform_train_test_split(qa_pair_output, \"train_split.csv\", \"test_split.csv\", FIXED_FIGURE_SAMPLE_SIZE, FIXED_TABLE_SAMPLE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417f70b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
